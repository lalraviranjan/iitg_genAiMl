{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b3cd53-f712-4235-9148-cf89908ee525",
   "metadata": {},
   "source": [
    "# Text - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79111c12-bf44-4089-b4f5-5728a0eddfd1",
   "metadata": {},
   "source": [
    "## **1. Tokenization**\n",
    "Tokenization is the process of breaking a **corpus (large text)** into **smaller meaningful units** like **sentences or words**.  \n",
    "\n",
    "**Types of Tokenization:**\n",
    "- **Sentence Tokenization** ‚Üí Splitting text into sentences.  \n",
    "- **Word Tokenization** ‚Üí Splitting sentences into words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c66ec5e-4706-4a32-9a1c-ace0e38e2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank.\n",
    "She had nothing to do. Once or twice, she peeped into the book her sister was reading.\n",
    "But it had no pictures or conversations in it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9294d28d-749c-410b-9124-7d31965d0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5ea7a2e-5d97-4970-88dd-30e9023c5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a corpus into sentences. using sentence tokenizer\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd848bae-cadb-4004-8c45-ac48b62ba549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f110af4e-6087-478d-9a1e-08691bb3b821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice was beginning to get very tired of sitting by her sister on the bank.',\n",
       " 'She had nothing to do.',\n",
       " 'Once or twice, she peeped into the book her sister was reading.',\n",
       " 'But it had no pictures or conversations in it.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0d5f944-9515-4736-89ce-ecf270e3825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank.\n",
      "She had nothing to do.\n",
      "Once or twice, she peeped into the book her sister was reading.\n",
      "But it had no pictures or conversations in it.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8e430a7-638b-4155-80a5-794d550c6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23e68ceb-fb16-48ae-b384-90a4d8e16270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank',\n",
       " '.',\n",
       " 'She',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'Once',\n",
       " 'or',\n",
       " 'twice',\n",
       " ',',\n",
       " 'she',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading',\n",
       " '.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize words\n",
    "word_token = word_tokenize(corpus)\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c357875-71d1-4a3e-ac15-3b15c3c1d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', '.']\n",
      "['She', 'had', 'nothing', 'to', 'do', '.']\n",
      "['Once', 'or', 'twice', ',', 'she', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', '.']\n",
      "['But', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    word_token = word_tokenize(sentence)\n",
    "    print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99d65275-bb50-4140-ace8-1b112880f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tree_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36c039a9-1e74-4ee7-983c-ef9bb1e6b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank.',\n",
       " 'She',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do.',\n",
       " 'Once',\n",
       " 'or',\n",
       " 'twice',\n",
       " ',',\n",
       " 'she',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treebank tokenizer will consider the word ending woth full stop with the full-stop ex- (bank.) while word tokenizer will seperate(bank) and (.)\n",
    "tree_tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53ce93bc-8f85-4136-acd0-459d79b7a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', '.']\n",
      "['She', 'had', 'nothing', 'to', 'do', '.']\n",
      "['Once', 'or', 'twice', ',', 'she', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', '.']\n",
      "['But', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(tree_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4d19-4c16-4111-85ed-cbed122588fb",
   "metadata": {},
   "source": [
    "## **2. Stemming (Reducing words to their root form by chopping endings)**\n",
    "**Definition:**  \n",
    "Stemming removes **prefixes or suffixes** from words to obtain their **root form** (also called the \"stem\").  \n",
    "\n",
    "üîπ **But it's a rule-based approach!** It simply cuts off word endings **without considering meaning**, which sometimes leads to incorrect words.  \n",
    "\n",
    "**Example:**  \n",
    "üìå Given words and their stems:  \n",
    "| Word | Stem (Incorrect sometimes) |\n",
    "|---|---|\n",
    "| Running | Run |\n",
    "| Studies | Studi ‚ùå |\n",
    "| Happily | Happili ‚ùå |\n",
    "| Better | Better ‚ùå (doesn't reduce properly) |\n",
    "\n",
    "**Downside:**  \n",
    "üö´ Since stemming only **chops off letters**, it may **not always return a real word**.  \n",
    "\n",
    "**Types of Stemming Algorithms:**  \n",
    "1. **Porter Stemmer** ‚Äì Simple, widely used.  \n",
    "2. **Lancaster Stemmer** ‚Äì More aggressive.  \n",
    "3. **Snowball Stemmer** ‚Äì More refined.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56e20ed3-96c4-4345-94c3-0d7af183376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example words\n",
    "words = [\"running\", \"flies\", \"happily\", \"studies\", \"cars\", \"better\", \"history\", \"fairly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e26c07e-c6eb-41de-b6bf-0a5baf2b23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f9130a0-a9ce-4bc0-b8bb-a04bb61051c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a1ceae5-f531-4326-8c6c-a4e9a565da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running--------->run\n",
      "flies--------->fli\n",
      "happily--------->happili\n",
      "studies--------->studi\n",
      "cars--------->car\n",
      "better--------->better\n",
      "history--------->histori\n",
      "fairly--------->fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word}--------->{stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855e1c5-d019-42d6-bf6b-10bedea12365",
   "metadata": {},
   "source": [
    "##### Major disadvantage in stemming is the meaning of the word changes - (history--------->histori), (studies--------->studi), this can be overcome by Lemmetization\n",
    "##### We can use other techniques of stemming to improve this like RegexpStemmer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a47cc-e2e4-491a-943e-1b5c378bdd07",
   "metadata": {},
   "source": [
    "#### RegexpStemmer\n",
    "The RegexpStemmer (Regular Expression Stemmer) in NLTK allows custom rule-based stemming using regex patterns. Instead of a predefined algorithm (like Porter or Lancaster), you define suffix removal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e80d3152-137a-4ea8-b512-3d5a1332ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dbc551d-605c-407f-a9c3-129763b15c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex says for ex wherever there is ing remove that so running gives runn\n",
    "regexp_stemming = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99a6e018-fd8e-41d4-b0b8-cad1675bc0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runn'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemming.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b18ebe18-15aa-4d89-a5f8-a773b888c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemming.stem(\"eatable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fae924-86f7-459d-87fc-704b50e526fc",
   "metadata": {},
   "source": [
    "#### SnowballStemmer\n",
    "The SnowballStemmer (also called \"Porter2 Stemmer\") is an improved version of the Porter Stemmer. It is more accurate, multilingual, and less aggressive than the Lancaster Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "405510a0-f6e3-424b-a8f2-4a074df614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "275cded4-f4f9-4500-8b2b-be3f20cc5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemming = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ccaec89-bc0f-4e7f-b13e-dca6ef83ae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemming.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "685f15d2-59d8-4a4d-bfb4-eb543c89f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemming.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40dcc4a1-52c6-4d7f-9d83-345b93243341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so here fairly is correctly transformed to its root word that is \"\"fair\" wheresas in PorterStemmer it is \"fairli\"\n",
    "snowball_stemming.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e8544-2138-47fa-8d1f-945ccf12f1e4",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "The Lancaster Stemmer is a very aggressive stemming algorithm that reduces words to their root forms but often over-stems, leading to loss of meaning. It is more aggressive than Porter and Snowball Stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "88466337-bc15-4def-b5a1-27ccb0266bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "367acd8e-7c2f-410e-8fdf-cadf97c52f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stemming = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9615a59-7915-46d4-9199-592bab9dff44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run', 'hist', 'fair')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemming.stem(\"running\"), lancaster_stemming.stem(\"history\"), lancaster_stemming.stem(\"fairly\") \n",
    "# its performs overstemming on history resulting in change of meaning or loosing the actual meaning of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215709d-e3da-4a7f-967a-2aedaf3cce5e",
   "metadata": {},
   "source": [
    "## **3. Lemmatization (Getting the dictionary base form of words)**\n",
    "**Definition:**  \n",
    "Lemmatization is an advanced version of stemming that reduces a word to its **meaningful base form (lemma)**, ensuring that it is a valid dictionary word.  \n",
    "\n",
    "**Difference from Stemming:**  \n",
    "‚úî **Lemmatization considers meaning and grammar**, while stemming blindly removes endings.  \n",
    "\n",
    "**Example:**  \n",
    "üìå Given words and their correct lemmas:  \n",
    "| Word | Lemma |\n",
    "|---|---|\n",
    "| Running | Run |\n",
    "| Studies | Study |\n",
    "| Happily | Happy |\n",
    "| Better | Good ‚úÖ (stemming fails here) |\n",
    "\n",
    "üîπ **Why is Lemmatization better?**  \n",
    "‚úî It ensures that the word remains meaningful.  \n",
    "‚úî Uses a vocabulary-based approach (WordNet, spaCy, etc.).  \n",
    "‚úî Helps in machine learning models where meaningful text is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c469f-ad8b-4c84-a457-a2cf0d0f74b0",
   "metadata": {},
   "source": [
    "#### Wordnet Lemmetizer\n",
    "The WordNet Lemmatizer is a more advanced technique than stemming.\n",
    "Nltk provides WordnetLemmatizer class which is think wrapper around the wornet corpus. This class uses morphy() function to the Wordnet Corpus Readder class to find a lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c03ab2ca-094b-4e9e-b738-8a58eeae618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "852993c1-9237-4c1e-8337-d3a15c075004",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "546cbeac-84a7-4c74-a309-c79d39b9c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos tag in Lemmetizer\n",
    "# the Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "#     `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "#     for satellite adjectives.\n",
    "lemmatizer.lemmatize(\"eating\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80bc4c86-f9a4-4596-8b34-38c563d33a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"running\", \"flies\", \"happily\", \"studies\", \"cars\", \"better\", \"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86b242ef-51f9-42af-bccb-b601d31172d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-------->running\n",
      "flies-------->fly\n",
      "happily-------->happily\n",
      "studies-------->study\n",
      "cars-------->car\n",
      "better-------->better\n",
      "history-------->history\n"
     ]
    }
   ],
   "source": [
    "# there will be not much stemming as pos is set to noun so only cars is lemmitized to car\n",
    "for word in words:\n",
    "    print(f\"{word}-------->{lemmatizer.lemmatize(word, pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a453c1d6-2ec1-43d1-aaf3-8769452974fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-------->run\n",
      "flies-------->fly\n",
      "happily-------->happily\n",
      "studies-------->study\n",
      "cars-------->cars\n",
      "better-------->better\n",
      "history-------->history\n"
     ]
    }
   ],
   "source": [
    "# performs stemming well as pos tagging is verb\n",
    "for word in words:\n",
    "    print(f\"{word}-------->{lemmatizer.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd8802-3eb1-4fbd-ac4b-0d2910f9fbb4",
   "metadata": {},
   "source": [
    "## **4. Stop-word Removal (Filtering out unimportant words)**\n",
    "**Definition:**  \n",
    "Stop-words are common words that **don‚Äôt add much meaning** to a sentence, like **‚Äúis‚Äù, ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúa‚Äù, ‚Äúin‚Äù**. Removing them helps **reduce noise** in NLP tasks.  \n",
    "\n",
    "**Example:**  \n",
    "üìå Given sentence:  \n",
    "> *\"The cat is sitting on the mat.\"*  \n",
    "\n",
    "‚úÖ After stop-word removal:  \n",
    "> *\"cat sitting mat\"*  \n",
    "\n",
    "üîπ **Why remove stop-words?**  \n",
    "‚úî Reduces text size for faster processing.  \n",
    "‚úî Focuses only on meaningful words.  \n",
    "‚úî Improves search engine results by removing unnecessary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "239287ef-64df-4753-83f3-0cfdec152211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2344ac3c-208d-46bd-a923-381a1a4b81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Stopwords in English = 198\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# these words in english / german / arabic etc in NLP are removed as an stopwords\n",
    "print(f\"Total Number of Stopwords in English = {len(stopwords.words(\"english\"))}\")\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb9ac78f-226f-4afe-bfc3-c7e1ec730acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 stopwords for ex\n",
    "stopwords.words(\"german\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4ea4c5b-8b34-4a64-bbf6-010b9391237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÿ•ÿ∞', 'ÿ•ÿ∞ÿß', 'ÿ•ÿ∞ŸÖÿß', 'ÿ•ÿ∞ŸÜ', 'ÿ£ŸÅ', 'ÿ£ŸÇŸÑ', 'ÿ£ŸÉÿ´ÿ±', 'ÿ£ŸÑÿß', 'ÿ•ŸÑÿß', 'ÿßŸÑÿ™Ÿä']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 stopwords for ex\n",
    "stopwords.words(\"arabic\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68527c17-7baa-433c-8f10-a9c261ddc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"My Vision for India ‚Äì Dr. A.P.J. Abdul Kalam. I have three visions for India.\n",
    "In 3000 years of history, India has been invaded, conquered, ruled by others. Yet, India has always stood strong, preserving its culture, knowledge, and traditions. The first vision is Freedom. We must protect our nation‚Äôs independence with our knowledge, innovation, and courage.  \n",
    "The second vision is Development. We must not be a developing nation forever. We have the potential to become a global leader in science, technology, and economy. We must believe in ourselves!  \n",
    "The third vision is India must stand up to the world. We must develop the mindset of a developed nation and not be looked upon as a third-world country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76d5af42-30f9-4068-89ea-cce99f07946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27b6b21a-5ddf-408d-9679-a0bc75c915f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentecnces = nltk.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49f16825-8314-4709-9dcb-fa42be436e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vision india ‚Äì dr. a.p.j .', 'abdul kalam .', 'three vision india .', '3000 year histori , india invad , conquer , rule other .', 'yet , india alway stood strong , preserv cultur , knowledg , tradit .', 'first vision freedom .', 'must protect nation ‚Äô independ knowledg , innov , courag .', 'second vision develop .', 'must develop nation forev .', 'potenti becom global leader scienc , technolog , economi .', 'must believ !', 'third vision india must stand world .', 'must develop mindset develop nation look upon third-world countri .']\n"
     ]
    }
   ],
   "source": [
    "## Apply Stopwords and filter and then apply stemming\n",
    "for i in range(len(sentecnces)):\n",
    "    words = nltk.word_tokenize(sentecnces[i].lower())\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentecnces[i] = ' '.join(words)\n",
    "print(sentecnces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "674729cc-7ffb-46b1-90f4-32bec6ecd897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vision india ‚Äì dr. a.p.j .', 'abdul kalam .', 'three vision india .', '3000 year histori , india invad , conquer , rule .', 'yet , india alway stood strong , preserv cultur , knowledg , tradit .', 'first vision freedom .', 'must protect nation ‚Äô independ knowledg , innov , courag .', 'second vision develop .', 'must develop nation forev .', 'potenti becom global leader scienc , technolog , economi .', 'must believ !', 'third vision india must stand world .', 'must develop mindset develop nation look upon third-world countri .']\n"
     ]
    }
   ],
   "source": [
    "## Apply Stopwords and filter and then apply lemmetization (better than stemming but takes more time)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(sentecnces)):\n",
    "    words = nltk.word_tokenize(sentecnces[i].lower())\n",
    "    words = [word_lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentecnces[i] = ' '.join(words)\n",
    "print(sentecnces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85492b-b355-44c9-af5b-2708b58dc954",
   "metadata": {},
   "source": [
    "## **Part-of-Speech (POS) Tagging** \n",
    "**POS tagging (Part-of-Speech tagging)** is an **NLP process** that labels each word in a sentence with its **grammatical role**, such as **noun, verb, adjective, pronoun, etc.** \n",
    "## **üîπ Why is POS Tagging Important?**  \n",
    "- **Sentence Structure Analysis** ‚Üí Helps understand the meaning of words in context.  \n",
    "- **Word Sense Disambiguation** ‚Üí Differentiates between words with multiple meanings (e.g., *\"book a ticket\"* vs. *\"read a book\"*).\n",
    "-  **NER (Named Entity Recognition)** ‚Üí Helps in recognizing entities based on their word type.\n",
    "-   **Grammar Correction & Speech Recognition** ‚Üí Used in spell-checking and voice assistants.\n",
    "-   **Lemmatization & Stemming** ‚Üí Helps identify the base form of words correctly.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **üîπ Common POS Tags in NLP**\n",
    "Here are some commonly used **POS tags** in NLP (based on the **Penn Treebank POS Tagset**):  \n",
    "\n",
    "| POS Tag | Full Form | Example |\n",
    "|---|---|---|\n",
    "| **NN** | Noun (Singular) | \"dog\", \"apple\" |\n",
    "| **NNS** | Noun (Plural) | \"dogs\", \"apples\" |\n",
    "| **NNP** | Proper Noun (Singular) | \"India\", \"Google\" |\n",
    "| **NNPS** | Proper Noun (Plural) | \"Indians\", \"Americans\" |\n",
    "| **VB** | Verb (Base Form) | \"run\", \"eat\" |\n",
    "| **VBD** | Verb (Past Tense) | \"ran\", \"ate\" |\n",
    "| **VBG** | Verb (Gerund/Present Participle) | \"running\", \"eating\" |\n",
    "| **VBN** | Verb (Past Participle) | \"eaten\", \"driven\" |\n",
    "| **VBP** | Verb (Singular Present) | \"run\", \"eat\" |\n",
    "| **VBZ** | Verb (3rd Person Singular Present) | \"runs\", \"eats\" |\n",
    "| **JJ** | Adjective | \"quick\", \"happy\" |\n",
    "| **RB** | Adverb | \"quickly\", \"happily\" |\n",
    "| **PRP** | Pronoun | \"he\", \"she\", \"it\" |\n",
    "| **IN** | Preposition | \"on\", \"in\", \"over\" |\n",
    "| **DT** | Determiner | \"the\", \"a\", \"an\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82fe1807-01d7-4567-9e9a-267c7e724cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f5bc4cf-3768-4f54-82ab-8080fb89be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vision', 'NN'), ('india', 'NN'), ('‚Äì', 'NNP'), ('dr.', 'NN'), ('a.p.j', 'NN'), ('.', '.')]\n",
      "[('abdul', 'JJ'), ('kalam', 'NN'), ('.', '.')]\n",
      "[('three', 'CD'), ('vision', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[('3000', 'CD'), ('year', 'NN'), ('histori', 'NN'), (',', ','), ('india', 'JJ'), ('invad', 'NN'), (',', ','), ('conquer', 'NN'), (',', ','), ('rule', 'NN'), ('.', '.')]\n",
      "[('yet', 'RB'), (',', ','), ('india', 'VB'), ('alway', 'RB'), ('stood', 'JJ'), ('strong', 'JJ'), (',', ','), ('preserv', 'JJ'), ('cultur', 'NN'), (',', ','), ('knowledg', 'NN'), (',', ','), ('tradit', 'NN'), ('.', '.')]\n",
      "[('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('protect', 'VB'), ('nation', 'NN'), ('‚Äô', 'NNP'), ('independ', 'NN'), ('knowledg', 'NN'), (',', ','), ('innov', 'NN'), (',', ','), ('courag', 'NN'), ('.', '.')]\n",
      "[('second', 'JJ'), ('vision', 'NN'), ('develop', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('develop', 'VB'), ('nation', 'NN'), ('forev', 'NN'), ('.', '.')]\n",
      "[('potenti', 'NN'), ('becom', 'NN'), ('global', 'JJ'), ('leader', 'NN'), ('scienc', 'NN'), (',', ','), ('technolog', 'NN'), (',', ','), ('economi', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('believ', 'VB'), ('!', '.')]\n",
      "[('third', 'JJ'), ('vision', 'NN'), ('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('develop', 'VB'), ('mindset', 'VB'), ('develop', 'VB'), ('nation', 'NN'), ('look', 'NN'), ('upon', 'IN'), ('third-world', 'JJ'), ('countri', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Find the Pos tag each words will be tagged to ie, verb adverb, adjective, noun etc as per post tag for ex ('india', 'NN') Inda as Noun\n",
    "for i in range(len(sentecnces)):\n",
    "    words = nltk.word_tokenize(sentecnces[i].lower())\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    tag_pos = pos_tag(words)\n",
    "    print(tag_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816eba2-6b62-4251-8ea2-ade90fceb9e8",
   "metadata": {},
   "source": [
    "## **Named Entity Recognition (NER) in NLP**  \n",
    "\n",
    "**Named Entity Recognition (NER)** is a Natural Language Processing (NLP) technique used to identify and classify **named entities** in text into predefined categories like:  \n",
    "\n",
    "- **Person** ‚Äì (\"A.P.J. Abdul Kalam\", \"Elon Musk\")\n",
    "- **Organization** ‚Äì (\"NASA\", \"Google\", \"ISRO\")\n",
    "- **Location** ‚Äì (\"India\", \"New York\", \"Himalayas\")\n",
    "- **Date & Time** ‚Äì (\"January 26, 1950\", \"5 PM\")\n",
    "- **Monetary Values** ‚Äì (\"‚Çπ1000\", \"$1 billion\")\n",
    "- **Percentages** ‚Äì (\"50%\", \"95% accuracy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a66f75e7-2d33-4c4a-aa53-9bb4d24f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"My Vision for India ‚Äì Dr. A.P.J. Abdul Kalam. I have three visions for India.\n",
    "In 3000 years of history, India has been invaded, conquered, ruled by others. Yet, India has always stood strong, preserving its culture, knowledge, and traditions. The first vision is Freedom. We must protect our nation‚Äôs independence with our knowledge, innovation, and courage.  \n",
    "The second vision is Development. We must not be a developing nation forever. We have the potential to become a global leader in science, technology, and economy. We must believe in ourselves!  \n",
    "The third vision is India must stand up to the world. We must develop the mindset of a developed nation and not be looked upon as a third-world country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "883935d9-2177-4694-838e-3053f3b97732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ac79de7c-5657-4b07-8e71-3d6703850b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(corpus)\n",
    "pos_tag_elements = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "790e483a-7566-4d50-9437-6cf8419333ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command nltk.ne_chunk(pos_tag_elements).draw() is used for Named Entity Recognition (NER) visualization in NLTK. It creates a tree structure showing named entities in the text.\n",
    "\n",
    "# uncomment this line to execute\n",
    "# nltk.ne_chunk(pos_tag_elements).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0783c-f8f6-425b-ad54-2e75aba082bd",
   "metadata": {},
   "source": [
    "## BOW (Bag of Words)\n",
    "---\n",
    "#### Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "876ecdee-9950-411f-9c31-1bf05a47f842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spam.csv',encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796331d-4817-40f7-939f-5a91a6d87434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e086c84-62b8-4449-af22-16e4bb87c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"v1\": \"label\", \"v2\": \"message\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0726e-a362-4348-a2ba-a9fed4b5d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d396d-7220-4e00-8cb2-540f5b50a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The re module in Python stands for Regular Expressions and is used for pattern matching and text processing.\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ef32e1e-32d2-4d4f-b536-f444bbea6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c232042-8e6c-496d-b924-7c22344be06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005366e5-9c6a-4abe-911e-2ea685d2fb64",
   "metadata": {},
   "source": [
    "1. **Removing special characters & numbers** using `re.sub(r\"[^a-zA-Z\\s]\", \"\", df['message'][i])`.  \n",
    "2. **Converting text to lowercase** with `.lower()`.  \n",
    "3. **Splitting text into words** using `.split()`.  \n",
    "4. **Applying stemming** (`port_stem(word)`) to reduce words to their root form.  \n",
    "5. **Removing stopwords** using `stopwords.words('english')`.  \n",
    "6. **Joining words back into a sentence** using `' '.join(review)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff48303-bf93-4cb4-a717-5d6fea733830",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub(r\"[^a-zA-Z\\s]\", \"\", df['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ss.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeda8a-d53f-42b7-bf11-5bbdc0867618",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe8111e-b918-4141-a037-8726b68f1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize CountVectorizer with a vocabulary size limit of 100 unique words\n",
    "# binary=True means each word presence is represented as 0 or 1 (instead of actual word counts)\n",
    "cv = CountVectorizer(max_features=100, binary=True)\n",
    "\n",
    "# If binary=False (default), the transformed values represent actual word frequencies # The below would store actual word counts instead of 0/1\n",
    "# cv = CountVectorizer(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7e259-5a6a-46ac-b548-a088db8ad4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the 'corpus' into a Bag of Words (BoW) representation\n",
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec40ab-0f1c-4feb-9b31-39687925b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix representation of BoW to a dense array\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867554e-bd99-4506-adfe-6fa6bceecb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the transformed dataset (rows = number of documents, columns = 100 words)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11604a-6eff-43fe-9a84-890ebad695c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 words with frquency\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af694c-4f1d-4787-91c8-2e7a1116a77c",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01bc82-9a61-417f-8d73-1ff748b6fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_range=(2,3)  -> Extracts both bigrams (2-word sequences) and trigrams (3-word sequences).\n",
    "# (1,2) - Combination of Unigram and bigram\n",
    "cv = CountVectorizer(max_features=100, binary=True, ngram_range=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321b092-e5f5-4840-bd1d-d915978dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd18ff-ee12-43a4-b94c-d2f5536f994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary dictionary where keys are n-grams and values are their index positions\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc2483-7ae0-4ca6-a60b-2991a47cad2b",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa1704-04e5-4971-b668-69220a3831fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize TfidfVectorizer with a max of 100 features (words)\n",
    "tfidf = TfidfVectorizer(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb2210-3dcc-4ab9-9072-9c81235a2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the corpus into TF-IDF representation\n",
    "X = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfbcc9-fdd8-461e-8a3b-07dd75e2d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62979b32-355b-46f1-b849-74828550a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary dictionary, where:\n",
    "# - Keys ‚Üí Words (features) selected by TF-IDF\n",
    "# - Values ‚Üí Their index positions in the feature matrix\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97d99b-dba3-45b8-b765-62d3632d01e8",
   "metadata": {},
   "source": [
    "#### TF-IDF with N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8c29f-813f-4e11-9db4-a113266e41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100, ngram_range=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692be969-06ed-4420-af52-a740041db1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184b206-4561-4a3e-8c85-a0e28d658c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba6851-66aa-4009-b461-1a89ca6e5e44",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation\n",
    "https://colab.research.google.com/drive/1DMK0Z3MM8D5st0-DdBmVdQFxTj8u-P0e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05965f36-7991-467b-a3fb-51da78ffb880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c23ce-718e-4d2e-893c-4585a5741574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b934d-0443-4429-9273-33ad2da90021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc283487-f134-4bc5-a7f7-71676a897c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083109f6-af22-4076-aabd-ee8264933a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33b62a-16b8-4af2-84a6-0bad61680db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47832509-ccc3-4394-8573-91b4f33b23bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0371e3e-d082-4d58-8837-6afa7239f5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8afea0-ea83-4f87-a16c-f18a0f5258d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519ece6-f78b-4a6e-8601-9bd2517b2087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
