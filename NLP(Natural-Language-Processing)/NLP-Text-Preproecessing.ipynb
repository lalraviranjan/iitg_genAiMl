{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b3cd53-f712-4235-9148-cf89908ee525",
   "metadata": {},
   "source": [
    "# Text - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79111c12-bf44-4089-b4f5-5728a0eddfd1",
   "metadata": {},
   "source": [
    "## **1. Tokenization**\n",
    "Tokenization is the process of breaking a **corpus (large text)** into **smaller meaningful units** like **sentences or words**.  \n",
    "\n",
    "**Types of Tokenization:**\n",
    "- **Sentence Tokenization** → Splitting text into sentences.  \n",
    "- **Word Tokenization** → Splitting sentences into words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0c66ec5e-4706-4a32-9a1c-ace0e38e2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank.\n",
    "She had nothing to do. Once or twice, she peeped into the book her sister was reading.\n",
    "But it had no pictures or conversations in it.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9294d28d-749c-410b-9124-7d31965d0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b5ea7a2e-5d97-4970-88dd-30e9023c5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a corpus into sentences. using sentence tokenizer\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cd848bae-cadb-4004-8c45-ac48b62ba549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0d5f944-9515-4736-89ce-ecf270e3825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank.\n",
      "She had nothing to do.\n",
      "Once or twice, she peeped into the book her sister was reading.\n",
      "But it had no pictures or conversations in it.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e8e430a7-638b-4155-80a5-794d550c6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "23e68ceb-fb16-48ae-b384-90a4d8e16270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank',\n",
       " '.',\n",
       " 'She',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'Once',\n",
       " 'or',\n",
       " 'twice',\n",
       " ',',\n",
       " 'she',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading',\n",
       " '.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize words\n",
    "word_token = word_tokenize(corpus)\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4c357875-71d1-4a3e-ac15-3b15c3c1d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', '.']\n",
      "['She', 'had', 'nothing', 'to', 'do', '.']\n",
      "['Once', 'or', 'twice', ',', 'she', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', '.']\n",
      "['But', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    word_token = word_tokenize(sentence)\n",
    "    print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "99d65275-bb50-4140-ace8-1b112880f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tree_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "36c039a9-1e74-4ee7-983c-ef9bb1e6b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank.',\n",
       " 'She',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do.',\n",
       " 'Once',\n",
       " 'or',\n",
       " 'twice',\n",
       " ',',\n",
       " 'she',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treebank tokenizer will consider the word ending woth full stop with the full-stop ex- (bank.) while word tokenizer will seperate(bank) and (.)\n",
    "tree_tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "53ce93bc-8f85-4136-acd0-459d79b7a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', '.']\n",
      "['She', 'had', 'nothing', 'to', 'do', '.']\n",
      "['Once', 'or', 'twice', ',', 'she', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', '.']\n",
      "['But', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(tree_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4d19-4c16-4111-85ed-cbed122588fb",
   "metadata": {},
   "source": [
    "## **2. Stemming (Reducing words to their root form by chopping endings)**\n",
    "**Definition:**  \n",
    "Stemming removes **prefixes or suffixes** from words to obtain their **root form** (also called the \"stem\").  \n",
    "\n",
    "🔹 **But it's a rule-based approach!** It simply cuts off word endings **without considering meaning**, which sometimes leads to incorrect words.  \n",
    "\n",
    "**Example:**  \n",
    "📌 Given words and their stems:  \n",
    "| Word | Stem (Incorrect sometimes) |\n",
    "|---|---|\n",
    "| Running | Run |\n",
    "| Studies | Studi ❌ |\n",
    "| Happily | Happili ❌ |\n",
    "| Better | Better ❌ (doesn't reduce properly) |\n",
    "\n",
    "**Downside:**  \n",
    "🚫 Since stemming only **chops off letters**, it may **not always return a real word**.  \n",
    "\n",
    "**Types of Stemming Algorithms:**  \n",
    "1. **Porter Stemmer** – Simple, widely used.  \n",
    "2. **Lancaster Stemmer** – More aggressive.  \n",
    "3. **Snowball Stemmer** – More refined.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "56e20ed3-96c4-4345-94c3-0d7af183376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example words\n",
    "words = [\"running\", \"flies\", \"happily\", \"studies\", \"cars\", \"better\", \"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5e26c07e-c6eb-41de-b6bf-0a5baf2b23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4f9130a0-a9ce-4bc0-b8bb-a04bb61051c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4a1ceae5-f531-4326-8c6c-a4e9a565da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running--------->run\n",
      "flies--------->fli\n",
      "happily--------->happili\n",
      "studies--------->studi\n",
      "cars--------->car\n",
      "better--------->better\n",
      "history--------->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word}--------->{stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855e1c5-d019-42d6-bf6b-10bedea12365",
   "metadata": {},
   "source": [
    "##### Major disadvantage in stemming is the meaning of the word changes - (history--------->histori), (studies--------->studi), this can be overcome by Lemmetization\n",
    "##### We can use other techniques of stemming to improve this like RegexpStemmer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a47cc-e2e4-491a-943e-1b5c378bdd07",
   "metadata": {},
   "source": [
    "#### RegexpStemmer\n",
    "The RegexpStemmer (Regular Expression Stemmer) in NLTK allows custom rule-based stemming using regex patterns. Instead of a predefined algorithm (like Porter or Lancaster), you define suffix removal rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e80d3152-137a-4ea8-b512-3d5a1332ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2dbc551d-605c-407f-a9c3-129763b15c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex says for ex wherever there is ing remove that so running gives runn\n",
    "regexp_stemming = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "99a6e018-fd8e-41d4-b0b8-cad1675bc0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runn'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemming.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b18ebe18-15aa-4d89-a5f8-a773b888c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_stemming.stem(\"eatable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fae924-86f7-459d-87fc-704b50e526fc",
   "metadata": {},
   "source": [
    "#### SnowballStemmer\n",
    "The SnowballStemmer (also called \"Porter2 Stemmer\") is an improved version of the Porter Stemmer. It is more accurate, multilingual, and less aggressive than the Lancaster Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "405510a0-f6e3-424b-a8f2-4a074df614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "275cded4-f4f9-4500-8b2b-be3f20cc5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemming = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6ccaec89-bc0f-4e7f-b13e-dca6ef83ae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemming.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "685f15d2-59d8-4a4d-bfb4-eb543c89f287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemming.stem(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "40dcc4a1-52c6-4d7f-9d83-345b93243341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemming.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e8544-2138-47fa-8d1f-945ccf12f1e4",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "The Lancaster Stemmer is a very aggressive stemming algorithm that reduces words to their root forms but often over-stems, leading to loss of meaning. It is more aggressive than Porter and Snowball Stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88466337-bc15-4def-b5a1-27ccb0266bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "367acd8e-7c2f-410e-8fdf-cadf97c52f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stemming = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b9615a59-7915-46d4-9199-592bab9dff44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run', 'hist', 'fair')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemming.stem(\"running\"), lancaster_stemming.stem(\"history\"), lancaster_stemming.stem(\"fairly\") \n",
    "# its performs overstemming on history resulting in change of meaning or loosing the actual meaning of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2215709d-e3da-4a7f-967a-2aedaf3cce5e",
   "metadata": {},
   "source": [
    "## **3. Lemmatization (Getting the dictionary base form of words)**\n",
    "**Definition:**  \n",
    "Lemmatization is an advanced version of stemming that reduces a word to its **meaningful base form (lemma)**, ensuring that it is a valid dictionary word.  \n",
    "\n",
    "**Difference from Stemming:**  \n",
    "✔ **Lemmatization considers meaning and grammar**, while stemming blindly removes endings.  \n",
    "\n",
    "**Example:**  \n",
    "📌 Given words and their correct lemmas:  \n",
    "| Word | Lemma |\n",
    "|---|---|\n",
    "| Running | Run |\n",
    "| Studies | Study |\n",
    "| Happily | Happy |\n",
    "| Better | Good ✅ (stemming fails here) |\n",
    "\n",
    "🔹 **Why is Lemmatization better?**  \n",
    "✔ It ensures that the word remains meaningful.  \n",
    "✔ Uses a vocabulary-based approach (WordNet, spaCy, etc.).  \n",
    "✔ Helps in machine learning models where meaningful text is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c469f-ad8b-4c84-a457-a2cf0d0f74b0",
   "metadata": {},
   "source": [
    "#### Wordnet Lemmetizer\n",
    "The WordNet Lemmatizer is a more advanced technique than stemming.\n",
    "Nltk provides WordnetLemmatizer class which is think wrapper around the wornet corpus. This class uses morphy() function to the Wordnet Corpus Readder class to find a lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c03ab2ca-094b-4e9e-b738-8a58eeae618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "852993c1-9237-4c1e-8337-d3a15c075004",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "546cbeac-84a7-4c74-a309-c79d39b9c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos tag in Lemmetizer\n",
    "# the Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
    "#     `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "#     for satellite adjectives.\n",
    "lemmatizer.lemmatize(\"eating\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "80bc4c86-f9a4-4596-8b34-38c563d33a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"running\", \"flies\", \"happily\", \"studies\", \"cars\", \"better\", \"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "86b242ef-51f9-42af-bccb-b601d31172d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-------->running\n",
      "flies-------->fly\n",
      "happily-------->happily\n",
      "studies-------->study\n",
      "cars-------->car\n",
      "better-------->better\n",
      "history-------->history\n"
     ]
    }
   ],
   "source": [
    "# there will be not much stemming as pos is set to noun so only cars is lemmitized to car\n",
    "for word in words:\n",
    "    print(f\"{word}-------->{lemmatizer.lemmatize(word, pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a453c1d6-2ec1-43d1-aaf3-8769452974fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running-------->run\n",
      "flies-------->fly\n",
      "happily-------->happily\n",
      "studies-------->study\n",
      "cars-------->cars\n",
      "better-------->better\n",
      "history-------->history\n"
     ]
    }
   ],
   "source": [
    "# performs stemming well as pos tagging is verb\n",
    "for word in words:\n",
    "    print(f\"{word}-------->{lemmatizer.lemmatize(word, pos='v')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd8802-3eb1-4fbd-ac4b-0d2910f9fbb4",
   "metadata": {},
   "source": [
    "## **4. Stop-word Removal (Filtering out unimportant words)**\n",
    "**Definition:**  \n",
    "Stop-words are common words that **don’t add much meaning** to a sentence, like **“is”, “the”, “and”, “a”, “in”**. Removing them helps **reduce noise** in NLP tasks.  \n",
    "\n",
    "**Example:**  \n",
    "📌 Given sentence:  \n",
    "> *\"The cat is sitting on the mat.\"*  \n",
    "\n",
    "✅ After stop-word removal:  \n",
    "> *\"cat sitting mat\"*  \n",
    "\n",
    "🔹 **Why remove stop-words?**  \n",
    "✔ Reduces text size for faster processing.  \n",
    "✔ Focuses only on meaningful words.  \n",
    "✔ Improves search engine results by removing unnecessary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "239287ef-64df-4753-83f3-0cfdec152211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2344ac3c-208d-46bd-a923-381a1a4b81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these words in english / german / arabic etc in NLP are removed as an stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cb9ac78f-226f-4afe-bfc3-c7e1ec730acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 stopwords for ex\n",
    "stopwords.words(\"german\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e4ea4c5b-8b34-4a64-bbf6-010b9391237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 10 stopwords for ex\n",
    "stopwords.words(\"arabic\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "68527c17-7baa-433c-8f10-a9c261ddc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"My Vision for India – Dr. A.P.J. Abdul Kalam. I have three visions for India.\n",
    "In 3000 years of history, India has been invaded, conquered, ruled by others. Yet, India has always stood strong, preserving its culture, knowledge, and traditions. The first vision is Freedom. We must protect our nation’s independence with our knowledge, innovation, and courage.  \n",
    "The second vision is Development. We must not be a developing nation forever. We have the potential to become a global leader in science, technology, and economy. We must believe in ourselves!  \n",
    "The third vision is India must stand up to the world. We must develop the mindset of a developed nation and not be looked upon as a third-world country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "76d5af42-30f9-4068-89ea-cce99f07946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "27b6b21a-5ddf-408d-9679-a0bc75c915f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentecnces = nltk.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "49f16825-8314-4709-9dcb-fa42be436e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vision india – dr. a.p.j .', 'abdul kalam .', 'three vision india .', '3000 year histori , india invad , conquer , rule .', 'yet , india alway stood strong , preserv cultur , knowledg , tradit .', 'first vision freedom .', 'must protect nation ’ independ knowledg , innov , courag .', 'second vision develop .', 'must develop nation forev .', 'potenti becom global leader scienc , technolog , economi .', 'must believ !', 'third vision india must stand world .', 'must develop mindset develop nation look upon third-world countri .']\n"
     ]
    }
   ],
   "source": [
    "## Apply Stopwords and filter and then apply stemming\n",
    "for i in range(len(sentecnces)):\n",
    "    words = nltk.word_tokenize(sentecnces[i].lower())\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentecnces[i] = ' '.join(words)\n",
    "print(sentecnces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85492b-b355-44c9-af5b-2708b58dc954",
   "metadata": {},
   "source": [
    "## **Part-of-Speech (POS) Tagging** \n",
    "**POS tagging (Part-of-Speech tagging)** is an **NLP process** that labels each word in a sentence with its **grammatical role**, such as **noun, verb, adjective, pronoun, etc.** \n",
    "## **🔹 Why is POS Tagging Important?**  \n",
    "- **Sentence Structure Analysis** → Helps understand the meaning of words in context.  \n",
    "- **Word Sense Disambiguation** → Differentiates between words with multiple meanings (e.g., *\"book a ticket\"* vs. *\"read a book\"*).\n",
    "-  **NER (Named Entity Recognition)** → Helps in recognizing entities based on their word type.\n",
    "-   **Grammar Correction & Speech Recognition** → Used in spell-checking and voice assistants.\n",
    "-   **Lemmatization & Stemming** → Helps identify the base form of words correctly.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **🔹 Common POS Tags in NLP**\n",
    "Here are some commonly used **POS tags** in NLP (based on the **Penn Treebank POS Tagset**):  \n",
    "\n",
    "| POS Tag | Full Form | Example |\n",
    "|---|---|---|\n",
    "| **NN** | Noun (Singular) | \"dog\", \"apple\" |\n",
    "| **NNS** | Noun (Plural) | \"dogs\", \"apples\" |\n",
    "| **NNP** | Proper Noun (Singular) | \"India\", \"Google\" |\n",
    "| **NNPS** | Proper Noun (Plural) | \"Indians\", \"Americans\" |\n",
    "| **VB** | Verb (Base Form) | \"run\", \"eat\" |\n",
    "| **VBD** | Verb (Past Tense) | \"ran\", \"ate\" |\n",
    "| **VBG** | Verb (Gerund/Present Participle) | \"running\", \"eating\" |\n",
    "| **VBN** | Verb (Past Participle) | \"eaten\", \"driven\" |\n",
    "| **VBP** | Verb (Singular Present) | \"run\", \"eat\" |\n",
    "| **VBZ** | Verb (3rd Person Singular Present) | \"runs\", \"eats\" |\n",
    "| **JJ** | Adjective | \"quick\", \"happy\" |\n",
    "| **RB** | Adverb | \"quickly\", \"happily\" |\n",
    "| **PRP** | Pronoun | \"he\", \"she\", \"it\" |\n",
    "| **IN** | Preposition | \"on\", \"in\", \"over\" |\n",
    "| **DT** | Determiner | \"the\", \"a\", \"an\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "82fe1807-01d7-4567-9e9a-267c7e724cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4f5bc4cf-3768-4f54-82ab-8080fb89be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vision', 'NN'), ('india', 'NN'), ('–', 'NNP'), ('dr.', 'NN'), ('a.p.j', 'NN'), ('.', '.')]\n",
      "[('abdul', 'JJ'), ('kalam', 'NN'), ('.', '.')]\n",
      "[('three', 'CD'), ('vision', 'NN'), ('india', 'NN'), ('.', '.')]\n",
      "[('3000', 'CD'), ('year', 'NN'), ('histori', 'NN'), (',', ','), ('india', 'JJ'), ('invad', 'NN'), (',', ','), ('conquer', 'NN'), (',', ','), ('rule', 'NN'), ('.', '.')]\n",
      "[('yet', 'RB'), (',', ','), ('india', 'VB'), ('alway', 'RB'), ('stood', 'JJ'), ('strong', 'JJ'), (',', ','), ('preserv', 'JJ'), ('cultur', 'NN'), (',', ','), ('knowledg', 'NN'), (',', ','), ('tradit', 'NN'), ('.', '.')]\n",
      "[('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('protect', 'VB'), ('nation', 'NN'), ('’', 'NNP'), ('independ', 'NN'), ('knowledg', 'NN'), (',', ','), ('innov', 'NN'), (',', ','), ('courag', 'NN'), ('.', '.')]\n",
      "[('second', 'JJ'), ('vision', 'NN'), ('develop', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('develop', 'VB'), ('nation', 'NN'), ('forev', 'NN'), ('.', '.')]\n",
      "[('potenti', 'NN'), ('becom', 'NN'), ('global', 'JJ'), ('leader', 'NN'), ('scienc', 'NN'), (',', ','), ('technolog', 'NN'), (',', ','), ('economi', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('believ', 'VB'), ('!', '.')]\n",
      "[('third', 'JJ'), ('vision', 'NN'), ('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('must', 'MD'), ('develop', 'VB'), ('mindset', 'VB'), ('develop', 'VB'), ('nation', 'NN'), ('look', 'NN'), ('upon', 'IN'), ('third-world', 'JJ'), ('countri', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Find the Pos tag each words will be tagged to ie, verb adverb, adjective, noun etc as per post tag for ex ('india', 'NN') Inda as Noun\n",
    "for i in range(len(sentecnces)):\n",
    "    words = nltk.word_tokenize(sentecnces[i].lower())\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    tag_pos = pos_tag(words)\n",
    "    print(tag_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816eba2-6b62-4251-8ea2-ade90fceb9e8",
   "metadata": {},
   "source": [
    "## **Named Entity Recognition (NER) in NLP**  \n",
    "\n",
    "**Named Entity Recognition (NER)** is a Natural Language Processing (NLP) technique used to identify and classify **named entities** in text into predefined categories like:  \n",
    "\n",
    "- **Person** – (\"A.P.J. Abdul Kalam\", \"Elon Musk\")\n",
    "- **Organization** – (\"NASA\", \"Google\", \"ISRO\")\n",
    "- **Location** – (\"India\", \"New York\", \"Himalayas\")\n",
    "- **Date & Time** – (\"January 26, 1950\", \"5 PM\")\n",
    "- **Monetary Values** – (\"₹1000\", \"$1 billion\")\n",
    "- **Percentages** – (\"50%\", \"95% accuracy\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a66f75e7-2d33-4c4a-aa53-9bb4d24f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus (plural: corpora) is a large collection of text data used for training, testing, and analyzing Natural Language Processing (NLP) models.\n",
    "corpus = \"\"\"My Vision for India – Dr. A.P.J. Abdul Kalam. I have three visions for India.\n",
    "In 3000 years of history, India has been invaded, conquered, ruled by others. Yet, India has always stood strong, preserving its culture, knowledge, and traditions. The first vision is Freedom. We must protect our nation’s independence with our knowledge, innovation, and courage.  \n",
    "The second vision is Development. We must not be a developing nation forever. We have the potential to become a global leader in science, technology, and economy. We must believe in ourselves!  \n",
    "The third vision is India must stand up to the world. We must develop the mindset of a developed nation and not be looked upon as a third-world country.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "883935d9-2177-4694-838e-3053f3b97732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\lalra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ac79de7c-5657-4b07-8e71-3d6703850b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(corpus)\n",
    "pos_tag_elements = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "790e483a-7566-4d50-9437-6cf8419333ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command nltk.ne_chunk(pos_tag_elements).draw() is used for Named Entity Recognition (NER) visualization in NLTK. It creates a tree structure showing named entities in the text.\n",
    "nltk.ne_chunk(pos_tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6b159-baa9-447a-84df-fc66effd42be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ecdee-9950-411f-9c31-1bf05a47f842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c796331d-4817-40f7-939f-5a91a6d87434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e086c84-62b8-4449-af22-16e4bb87c976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0726e-a362-4348-a2ba-a9fed4b5d861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d396d-7220-4e00-8cb2-540f5b50a186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef32e1e-32d2-4d4f-b536-f444bbea6117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e81aa-32da-478f-be8b-f58a563af9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f83e4-d3ec-4c6c-8b78-5815d7c39041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e0571-ab12-4dda-bd38-e592f483ded6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
