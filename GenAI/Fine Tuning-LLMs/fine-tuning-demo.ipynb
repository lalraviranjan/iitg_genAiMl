{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a10545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836711c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# financial qna dataset\n",
    "# https://huggingface.co/datasets/ibm-research/finqa\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ibm-research/finqa\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1673f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pre_text', 'post_text', 'table', 'question', 'answer', 'final_result', 'program_re', 'gold_inds'],\n",
       "    num_rows: 6251\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a124595",
   "metadata": {},
   "source": [
    "##### üîß What Does This Do?\n",
    "\n",
    "```python\n",
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"text\": f\"### Question: {example['question']}\\n### Context: {example['table']}\\n### Answer: {example['answer']}\"\n",
    "}\n",
    "\n",
    "dataset = dataset.map(format_instruction)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### üí° Purpose:\n",
    "\n",
    "This function **reformats the raw dataset entries into a text format that matches what you want the LLM to learn** ‚Äî using **instruction-style prompts**.\n",
    "\n",
    "---\n",
    "\n",
    "##### üß† Step-by-Step Explanation:\n",
    "**Formatted Output (Instruction Tuning Style):**\n",
    "\n",
    "   ```python\n",
    "   \"text\": f\"### Question: {example['question']}\\n### Context: {example['table']}\\n### Answer: {example['answer']}\"\n",
    "   ```\n",
    "\n",
    "   * This creates a string that clearly separates:\n",
    "\n",
    "     * The **question** being asked\n",
    "     * The **context** (the financial table relevant to answering it)\n",
    "     * The **expected answer**\n",
    "   * You‚Äôre turning structured fields into one long, readable **prompt-response training pair**.\n",
    "\n",
    "   Example output:\n",
    "\n",
    "   ```\n",
    "   ### Question: What is the net income for Q4?\n",
    "   ### Context: {\"Revenue\": \"500M\", \"Cost\": \"300M\", \"Quarter\": \"Q4\"}\n",
    "   ### Answer: 200M\n",
    "   ```\n",
    "\n",
    "**Mapping Over the Dataset:**\n",
    "\n",
    "   ```python\n",
    "   dataset = dataset.map(format_instruction)\n",
    "   ```\n",
    "\n",
    "   * Applies the formatting function to **every example** in your dataset.\n",
    "   * Adds a `\"text\"` field with the formatted string that can be used directly for fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Why Is This Important?\n",
    "\n",
    "* LLMs are trained on **natural language prompts**.\n",
    "* Your raw dataset is **structured** (with fields like `question`, `table`, `answer`) ‚Äî not immediately usable by a model like Mistral.\n",
    "* By reformatting, you're turning it into **instruction tuning format** (like how OpenAI instructs ChatGPT).\n",
    "\n",
    "---\n",
    "\n",
    "##### üèÅ What Happens After This?\n",
    "\n",
    "Once this `\"text\"` field is prepared:\n",
    "\n",
    "* You **tokenize** this field (`dataset[\"text\"]`) using a tokenizer.\n",
    "* Then, you **fine-tune the model** to learn to map from question + context ‚Üí answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a16ad5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7093e2b8d6b94318942729c87b243086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"text\": f\"### Question: {example['question']}\\n### Context: {example['table']}\\n### Answer: {example['answer']}\"\n",
    "}\n",
    "\n",
    "dataset = dataset.map(format_instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e11b9",
   "metadata": {},
   "source": [
    "##### Load Model with LoRA Support (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cd0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccdd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820aafa33c0543b0a2ca9afa5e090460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "# choose a good base model for finetuning\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Configure bitsandbytes for 4-bit quantization to reduce memory usage and enable faster inference/training\n",
    "# Load the model weights in 4-bit precision\n",
    "# Use float16 for computation to balance speed and accuracy\n",
    "# Enable double quantization for better compression and performance\n",
    "# Use \"nf4\" (Normal Float 4) for efficient quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                         \n",
    "    bnb_4bit_compute_dtype=torch.float16,       \n",
    "    bnb_4bit_use_double_quant=True,             \n",
    "    bnb_4bit_quant_type=\"nf4\"                   \n",
    ")\n",
    "\n",
    "# Load the tokenizer for the specified model\n",
    "# 'use_fast=True' enables the use of the fast Rust-based tokenizer (recommended for speed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Load the pre-trained Causal Language Model with quantization and automatic device mapping\n",
    "# device_map=\"auto\" ensures the model is spread across available GPUs/CPUs efficiently\n",
    "# Automatically decide device placement for layers (e.g., GPU/CPU)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aaefab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 4.23515136 GB\n",
      "Reserved:  4.737466368 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1e9, \"GB\")\n",
    "print(\"Reserved: \", torch.cuda.memory_reserved() / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128d33c",
   "metadata": {},
   "source": [
    "#### Configure LoRA (Using Rank 8, Alph 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import PEFT (Parameter-Efficient Fine-Tuning) and LoRA configuration\n",
    "# LoRA (Low-Rank Adaptation) enables fine-tuning only a small number of additional parameters \n",
    "# instead of updating the full model weights, making training efficient and lightweight.\n",
    "# We do so because large language models have billions of parameters and training them fully is resource-intensive.\n",
    "# LoRA injects trainable low-rank matrices into certain layers (like attention), reducing memory usage and training time.\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the original model.\n",
    "# This wraps the base model with LoRA adapters so only the relevant weights are made trainable.\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print the number of trainable parameters compared to total parameters in the model.\n",
    "# This helps verify that LoRA is working correctly by updating only a small portion of the model.\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db7961",
   "metadata": {},
   "source": [
    "#### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff9724",
   "metadata": {},
   "source": [
    "\n",
    "##### üîÑ Tokenization vs. Embedding\n",
    "\n",
    "##### üîπ 1. **Tokenization**\n",
    "\n",
    "* Converts raw **text ‚Üí tokens ‚Üí token IDs (integers)**.\n",
    "* Example:\n",
    "\n",
    "  ```\n",
    "  Text: \"What is ROI?\"\n",
    "  Tokens: [\"What\", \"is\", \"ROI\", \"?\"]\n",
    "  Token IDs: [1547, 318, 2543, 30]\n",
    "  ```\n",
    "* This step is **required before** feeding data into any LLM.\n",
    "* You do this using the model's **AutoTokenizer**.\n",
    "\n",
    "##### üîπ 2. **Embedding**\n",
    "\n",
    "* Converts token IDs into **dense vector representations (floats)** in high-dimensional space.\n",
    "* It‚Äôs the first learned layer inside the LLM.\n",
    "* OpenAIEmbeddings (like in `langchain.embeddings.OpenAIEmbeddings`) produce these vectors to **measure semantic similarity**, often used in:\n",
    "\n",
    "  * RAG pipelines\n",
    "  * Search\n",
    "  * Clustering\n",
    "  * Similarity scoring\n",
    "\n",
    "---\n",
    "\n",
    "###### ‚ö†Ô∏è Key Differences:\n",
    "\n",
    "| Feature      | Tokenization                           | Embedding (OpenAIEmbeddings etc.)              |\n",
    "| ------------ | -------------------------------------- | ---------------------------------------------- |\n",
    "| Converts     | Text ‚Üí token IDs                       | Token IDs ‚Üí dense vectors (e.g., 1536 dims)    |\n",
    "| Used in      | Preprocessing for model training/infer | Semantic search / similarity / vector stores   |\n",
    "| Produces     | Integers (ids)                         | Float vectors                                  |\n",
    "| Required for | LLM model training/fine-tuning         | RAG, vector search (not LLM training directly) |\n",
    "| Library used | `AutoTokenizer` from Transformers      | `OpenAIEmbeddings`, `HuggingFaceEmbeddings`    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0b5da",
   "metadata": {},
   "source": [
    "##### We are using the **tokenizer that was trained alongside the Mistral model**, which ensures:\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ **Why using the same tokenizer as the model is critical:**\n",
    "\n",
    "| Reason                 | Explanation                                                                                                                                                             |\n",
    "| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Vocabulary match**   | Mistral was trained using a specific tokenizer with its own vocabulary. Using a different tokenizer may produce token IDs that don't align with what the model expects. |\n",
    "| **Token ID alignment** | Mismatched token IDs ‚Üí unpredictable output or degraded performance during fine-tuning/inference.                                                                       |\n",
    "| **Special tokens**     | The tokenizer also handles special tokens (`<pad>`, `<bos>`, `<eos>`) which are model-specific.                                                                         |\n",
    "| **Formatting**         | Models like Mistral often use specific prompts or separator tokens ‚Äî the tokenizer ensures they're handled properly.                                                    |\n",
    "\n",
    "---\n",
    "\n",
    "##### üß† Pro Tip:\n",
    "\n",
    "You should **always** match the tokenizer and model versions (Mistral ‚Üí Mistral's tokenizer) unless you're doing research with multi-tokenizer settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92654246689e4887932cdeddf84d946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a tokenize function to preprocess the dataset\n",
    "# We apply truncation and padding to ensure all sequences are of the same length (512 tokens here),\n",
    "# which is necessary for batch training and fits within memory limits of the model.\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Set the tokenizer's padding token to the end-of-sequence token (EOS)\n",
    "# We do so because some pretrained models (like Mistral) may not have a defined pad token.\n",
    "# This prevents errors during padding and ensures consistency in sequence endings.\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Apply the tokenize function to the entire dataset using batched processing\n",
    "# This transforms all samples into tokenized format (input_ids, attention_mask), ready for training.\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070f06e",
   "metadata": {},
   "source": [
    "##### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4874efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient checkpointing to save GPU memory during backpropagation\n",
    "# This trades compute for memory by re-computing intermediate activations on the fly\n",
    "# Especially useful when fine-tuning large models on limited VRAM (e.g., 8GB GPUs)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Ensure input embeddings have `requires_grad=True` \n",
    "# This is necessary when using PEFT (e.g., LoRA) so gradients are correctly computed for adapter layers\n",
    "# Without this, you might encounter `RuntimeError: element 0 of tensors does not require grad...`\n",
    "\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d39889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='4686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  80/4686 06:09 < 6:03:36, 0.21 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.176800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     14\u001b[39m data_collator = DataCollatorForLanguageModeling(\n\u001b[32m     15\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     16\u001b[39m     mlm=\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# since we're training a causal language model like Mistral\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m trainer = Trainer(\n\u001b[32m     20\u001b[39m     model=model,\n\u001b[32m     21\u001b[39m     args=training_args,\n\u001b[32m     22\u001b[39m     train_dataset=tokenized_dataset,\n\u001b[32m     23\u001b[39m     data_collator=data_collator\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\transformers\\trainer.py:3782\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3780\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3782\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\accelerate\\accelerator.py:2450\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2449\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2450\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2451\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Set up the training arguments for the Hugging Face Trainer API\n",
    "# - output_dir: where to save the model\n",
    "# - per_device_train_batch_size: smaller batch to fit in limited VRAM\n",
    "# - gradient_accumulation_steps: accumulate gradients over multiple steps to simulate a larger batch size\n",
    "# - fp16: enable mixed precision training for faster performance and lower memory use\n",
    "# - save_strategy: save model at the end of every epoch\n",
    "# - report_to: disables logging to external tools (e.g., WandB)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_mistral\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save GPU memory by recomputing intermediate activations\n",
    "# Especially helpful when fine-tuning large models with limited resources\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Create a data collator for Causal Language Modeling (CLM)\n",
    "# - mlm=False: disables masked language modeling (used for BERT-style training)\n",
    "# - Suitable for models like Mistral, GPT which are trained as autoregressive generators\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer API with model, data, and training args\n",
    "# - Trainer handles training loop, gradient accumulation, logging, saving, etc.\n",
    "# - Works well with PEFT/LoRA for efficient fine-tuning\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e8c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_mistral_lora\\\\tokenizer_config.json',\n",
       " './finetuned_mistral_lora\\\\special_tokens_map.json',\n",
       " './finetuned_mistral_lora\\\\tokenizer.model',\n",
       " './finetuned_mistral_lora\\\\added_tokens.json',\n",
       " './finetuned_mistral_lora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save LoRA adapter\n",
    "# model.save_pretrained(\"./finetuned_mistral_lora\")\n",
    "# tokenizer.save_pretrained(\"./finetuned_mistral_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f13092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lalra\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned_mistral_merged\\\\tokenizer_config.json',\n",
       " './finetuned_mistral_merged\\\\special_tokens_map.json',\n",
       " './finetuned_mistral_merged\\\\tokenizer.model',\n",
       " './finetuned_mistral_merged\\\\added_tokens.json',\n",
       " './finetuned_mistral_merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the LoRA adapter weights into the base model weights\n",
    "# This step is necessary before exporting the model for standalone inference\n",
    "# After merging, the model no longer depends on the PEFT (LoRA) framework\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model to disk so it can be used without PEFT/LoRA at inference time\n",
    "# The saved model directory will include the model config and weights\n",
    "merged_model.save_pretrained(\"./finetuned_mistral_merged\")\n",
    "\n",
    "# Save the tokenizer associated with the model\n",
    "# This ensures consistent tokenization during inference\n",
    "tokenizer.save_pretrained(\"./finetuned_mistral_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is the total revenue in 2022?\n",
      "### Context: <your table>\n",
      "\n",
      "To find the total revenue in 2022, we need to sum up the revenue for each month in 2022. Here's how you can do it using SQL:\n",
      "\n",
      "```sql\n",
      "SELECT SUM(revenue) AS TotalRevenue\n",
      "FROM your_table\n",
      "WHERE YEAR(date_column) = 2022;\n",
      "```\n",
      "\n",
      "Replace `your_table` with the name of your table and\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input prompt in the format used during fine-tuning\n",
    "# Use special markers like ### Question and ### Context to guide the model‚Äôs response\n",
    "# Replace <your table> with the actual table/contextual information from your domain\n",
    "input_text = \"### Question: What is the total revenue in 2022?\\n### Context: <your table>\"\n",
    "\n",
    "# Tokenize the input and move it to the GPU (cuda) for inference\n",
    "# 'return_tensors=\"pt\"' converts input into PyTorch tensors suitable for model input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate the model's response using causal language modeling\n",
    "# 'max_new_tokens' controls how many tokens the model can generate in its answer\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode the output tokens into readable text\n",
    "# 'skip_special_tokens=True' removes any padding or special tokens used during generation\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
