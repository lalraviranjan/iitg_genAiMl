{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88fe694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334f6812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250412+cu128\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca88d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250412+cu128\n",
      "CUDA available: True\n",
      "cuDNN version: 90701\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93087e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline, AutoTokenizer, and AutoModelForCausalLM from transformers\n",
    "# pipeline : this is a high-level API for using models easily, it gives you a simple interface to use models for various tasks. example: text generation, translation, etc.\n",
    "# AutoTokenizer : this is used to convert text into tokens that the model can understand. It handles the preprocessing of text data.\n",
    "# AutoModelForCausalLM : this is a class for loading pre-trained models for causal language modeling tasks. It allows you to use models like GPT-2, GPT-3, etc.\n",
    "\n",
    "# Import the BitsAndBytesConfig class for quantization. \n",
    "# BitsAndBytesConfig : this is used for configuring quantization settings for models. It helps in reducing the model size and improving inference speed without significant loss in performance.\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ab64f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7b904c811f4e8fba455b20fa4e8eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b737fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model name from Hugging Face\n",
    "# # model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# # Configure 4-bit quantization\n",
    "# quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# # Load model with quantization and automatic device placement\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=quant_config,\n",
    "#     device_map=\"auto\" # Automatically place model on available devices (CPU/GPU)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9818f",
   "metadata": {},
   "source": [
    "\n",
    "# **What the progress bar shows**\n",
    "\n",
    "You're running:\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "This line tells Hugging Face to:\n",
    "\n",
    "1. **Download the model weights and config** from the Hub.\n",
    "2. **Quantize the model (4-bit in your case)** using `BitsAndBytesConfig`.\n",
    "3. **Place it on the correct device** (`device_map=\"auto\"` handles that).\n",
    "\n",
    "---\n",
    "\n",
    "### The progress bars:\n",
    "\n",
    "#### `config.json`  \n",
    "- This file defines the model architecture and tokenizer setup.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ `model.safetensors.index.json`  \n",
    "- This tells Hugging Face how to **split and reference** the model weights.\n",
    "- Since these big models are often too large for a single file, they’re split into chunks (`model-00001`, `00002`, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "#### 📦 The 3 large files:\n",
    "- `model-00001-of-00003.safetensors`  \n",
    "- `model-00002-of-00003.safetensors`  \n",
    "- `model-00003-of-00003.safetensors`\n",
    "\n",
    "Each of these is a **partial weight file** (chunks of the full model parameters):\n",
    "- You're downloading ~15 GB total (around 5 GB each).\n",
    "- The download is in progress, shows speed and ETA for each file.\n",
    "\n",
    "---\n",
    "\n",
    "### The warning:\n",
    "\n",
    "```txt\n",
    "UserWarning: 'huggingface_hub' cannot create symlinks...\n",
    "```\n",
    "\n",
    "It means:\n",
    "- On Windows, creating \"symlinks\" (shortcut-like references) is restricted unless you're in **Developer Mode** or running Python as an **Administrator**.\n",
    "- Hugging Face uses symlinks sometimes to save disk space.\n",
    "\n",
    "---\n",
    "\n",
    "### In Summary\n",
    "\n",
    "You're successfully:\n",
    "- Loading the Mistral 7B Instruct model\n",
    "- With 4-bit quantization\n",
    "- While downloading its 3 weight shards\n",
    "\n",
    "And you will be able to run inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3a767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the theory of relativity.\n",
      "\n",
      "The theory of relativity, proposed by Albert Einstein in 1905 and 1915, is a fundamental theory in physics that describes the behavior of objects and energy in the universe. It consists of two parts: special relativity and general relativity.\n",
      "\n",
      "1. Special Relativity: This theory describes the relationship between space and time at constant velocities, where the velocity of the observer is negligible compared to the velocity of light. The key principles of special relativity are:\n",
      "\n",
      "   a. The Principle of Relativity: The laws of physics are the same for all observers in uniform relative motion.\n",
      "\n",
      "   b. The Speed of Light is Constant: The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the source of light.\n",
      "\n",
      "   c. Time Dilation and Length Contraction: As an object approaches the speed of light, time appears to slow down for observers on Earth (time dilation), and the length of the object contracts (length contraction).\n",
      "\n",
      "2. General Relativity: This theory extends special relativity to include acceleration and gravity. It describes gravity not as a force, but as a curvature of spacetime caused by mass and energy. The key principles of general relativity are:\n",
      "\n",
      "   a. Gravitational Mass and Inertial Mass are Equivalent: The force of gravity on an\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the model. This is inference, where we provide input text to the model and get a generated response.\n",
    "input_text = \"Explain the theory of relativity.\"\n",
    "# Tokenize the input text and move it to the same device as the model\n",
    "# The tokenizer converts the input text into a format that the model can understand (tokens).\n",
    "# The return_tensors=\"pt\" argument specifies that the output should be in PyTorch tensor format.\n",
    "\n",
    "# The model is then used to generate text based on the input tokens.\n",
    "# The max_new_tokens argument specifies the maximum number of tokens to generate in the output.\n",
    "\n",
    "# Finally, the generated tokens are decoded back into text format using the tokenizer's decode method.\n",
    "# The skip_special_tokens=True argument ensures that special tokens (like padding or end-of-sequence tokens) are not included in the final output.\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f8bfa",
   "metadata": {},
   "source": [
    "#### Command to view for real time GPU utilization in Command Prompt `nvidia-smi -l 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290f2a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110379ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
