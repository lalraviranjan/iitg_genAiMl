{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88fe694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch used for the torchscript model, where GPU acceleration is needed, which helps to speed up the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca88d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0.dev20250412+cu128\n",
      "CUDA available: True\n",
      "cuDNN version: 90701\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "# Check if torch is compiled with CUDA, meaning it can use GPU acceleration\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# Check if torch backends are compiled with CUDA, and cuDNN is available in the system\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "# Check the number of GPUs available, this confirms GPUs are detected and available for use\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93087e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline, AutoTokenizer, and AutoModelForCausalLM from transformers\n",
    "# pipeline : this is a high-level API for using models easily, it gives you a simple interface to use models for various tasks. example: text generation, translation, etc.\n",
    "# AutoTokenizer : this is used to convert text into tokens that the model can understand. It handles the preprocessing of text data.\n",
    "# AutoModelForCausalLM : this is a class for loading pre-trained models for causal language modeling tasks. It allows you to use models like GPT-2, GPT-3, etc.\n",
    "\n",
    "# Import the BitsAndBytesConfig class for quantization. \n",
    "# BitsAndBytesConfig : this is used for configuring quantization settings for models. It helps in reducing the model size and improving inference speed without significant loss in performance.\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ab64f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653f2af7b81d4a258434033b8e512c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Load the model with 4-bit quantization using the BitsAndBytesConfig class\n",
    "# The model is loaded with 4-bit quantization to reduce memory usage and improve inference speed.\n",
    "# load_in_4bit : this parameter indicates that the model should be loaded in 4-bit precision.\n",
    "# bnb_4bit_use_double_quant : this parameter indicates whether to use double quantization for better performance. (Double quantization is a technique that helps in reducing the model size further while maintaining performance.)\n",
    "# bnb_4bit_quant_type : this parameter specifies the type of quantization to be used. \"nf4\" is a specific quantization type that is optimized for performance.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the model using the AutoModelForCausalLM class with the specified model ID and quantization configuration.\n",
    "# device_map : this parameter indicates how to distribute the model across available devices. \"auto\" automatically places the model on available devices (CPU/GPU).\n",
    "# The model is loaded with the specified quantization configuration to reduce memory usage and improve inference speed.\n",
    "# This model is loaded in GPU memory and everytime we call the model, it will be loaded in GPU memory. It then uses the GPU for inference.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # Automatically place model on available devices (CPU/GPU)\n",
    ")\n",
    "\n",
    "# Load the tokenizer using the AutoTokenizer class with the specified model ID.\n",
    "# The tokenizer is responsible for converting text into tokens that the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9818f",
   "metadata": {},
   "source": [
    "\n",
    "# **What the progress bar shows**\n",
    "\n",
    "You're running:\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(...)\n",
    "```\n",
    "\n",
    "This line tells Hugging Face to:\n",
    "\n",
    "1. **Download the model weights and config** from the Hub.\n",
    "2. **Quantize the model (4-bit in your case)** using `BitsAndBytesConfig`.\n",
    "3. **Place it on the correct device** (`device_map=\"auto\"` handles that).\n",
    "\n",
    "---\n",
    "\n",
    "### The progress bars:\n",
    "\n",
    "#### `config.json`  \n",
    "- This file defines the model architecture and tokenizer setup.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ `model.safetensors.index.json`  \n",
    "- This tells Hugging Face how to **split and reference** the model weights.\n",
    "- Since these big models are often too large for a single file, they’re split into chunks (`model-00001`, `00002`, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "#### 📦 The 3 large files:\n",
    "- `model-00001-of-00003.safetensors`  \n",
    "- `model-00002-of-00003.safetensors`  \n",
    "- `model-00003-of-00003.safetensors`\n",
    "\n",
    "Each of these is a **partial weight file** (chunks of the full model parameters):\n",
    "- You're downloading ~15 GB total (around 5 GB each).\n",
    "- The download is in progress, shows speed and ETA for each file.\n",
    "\n",
    "---\n",
    "\n",
    "### The warning:\n",
    "\n",
    "```txt\n",
    "UserWarning: 'huggingface_hub' cannot create symlinks...\n",
    "```\n",
    "\n",
    "It means:\n",
    "- On Windows, creating \"symlinks\" (shortcut-like references) is restricted unless you're in **Developer Mode** or running Python as an **Administrator**.\n",
    "- Hugging Face uses symlinks sometimes to save disk space.\n",
    "\n",
    "---\n",
    "\n",
    "### In Summary\n",
    "\n",
    "You're successfully:\n",
    "- Loading the Mistral 7B Instruct model\n",
    "- With 4-bit quantization\n",
    "- While downloading its 3 weight shards\n",
    "\n",
    "And you will be able to run inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3a767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the theory of relativity.\n",
      "\n",
      "The theory of relativity, proposed by Albert Einstein in 1905 and 1915, is a fundamental theory in physics that describes the behavior of objects and energy in the universe. It consists of two parts: special relativity and general relativity.\n",
      "\n",
      "1. Special Relativity: This theory describes the relationship between space and time at constant velocities, where the velocity of the observer is negligible compared to the velocity of light. The key principles of special relativity are:\n",
      "\n",
      "   a. The Principle of Relativity: The laws of physics are the same for all observers in uniform relative motion.\n",
      "\n",
      "   b. The Speed of Light is Constant: The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the source of light.\n",
      "\n",
      "   c. Time Dilation and Length Contraction: As an object approaches the speed of light, time appears to slow\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the model. This is inference, where we provide input text to the model and get a generated response.\n",
    "input_text = \"Explain the theory of relativity.\"\n",
    "# Tokenize the input text and move it to the same device as the model\n",
    "# The tokenizer converts the input text into a format that the model can understand (tokens).\n",
    "# The return_tensors=\"pt\" argument specifies that the output should be in PyTorch tensor format.\n",
    "\n",
    "# The model is then used to generate text based on the input tokens.\n",
    "# The max_new_tokens argument specifies the maximum number of tokens to generate in the output.\n",
    "\n",
    "# Finally, the generated tokens are decoded back into text format using the tokenizer's decode method.\n",
    "# The skip_special_tokens=True argument ensures that special tokens (like padding or end-of-sequence tokens) are not included in the final output.\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f8bfa",
   "metadata": {},
   "source": [
    "#### Command to view for real time GPU utilization in Command Prompt `nvidia-smi -l 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66168c58",
   "metadata": {},
   "source": [
    "#### Show total GPU memory reserved and allocated and current model loaded to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110379ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 4.149719552 GB\n",
      "Reserved:  4.5088768 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Allocated basically means the model is using this much memory\n",
    "# Reserved means the memory is reserved for PyTorch for inference, but not all of it is being used by the model.\n",
    "print(\"Allocated:\", torch.cuda.memory_allocated() / 1e9, \"GB\")\n",
    "print(\"Reserved: \", torch.cuda.memory_reserved() / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fdc8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM\n"
     ]
    }
   ],
   "source": [
    "#Current Loaded model\n",
    "print(model.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
