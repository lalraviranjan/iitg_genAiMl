{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170bddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Loads environment variables from a .env file into your system's environment.\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b913b1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000025FF32B5A00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025FF32B6BA0>, model_name='Gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(api_key=groq_api_key, model=\"Gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd0032",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "Prompt Template helps to turn raw user information into a format that the LLM (Large Language Model) can understand and reason over effectively.\n",
    "It allows developers to structure inputs using placeholders and inject dynamic values (like user queries, retrieved documents, or memory) into a predefined message layout â€” ensuring consistent, contextual, and optimized prompts for better responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e872f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# The \"system\" message sets the role and behavior of the assistant, providing context for how it should respond.\n",
    "# MessagesPlaceholder is a dynamic placeholder that allows injecting a list of messages (conversation history) at runtime.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are helpful assistant. Answer all the question to the best of your ability in this {language}.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt|model|parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff130ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# This is an in-memory store that maps each unique session ID (like a user's chat session) to its own ChatMessageHistory.\n",
    "# Think of it like a cache that holds ongoing conversations by session_id.\n",
    "store = {}\n",
    "\n",
    "# This function checks if a message history exists for the given session, If not, it initializes a new ChatMessageHistory() object and stores it in store using the session_id as the key\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f428cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session config, It creates a config dictionary that is passed to .invoke() in RunnableWithMessageHistory. This tells LangChain which conversation session to use\n",
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1074c7",
   "metadata": {},
   "source": [
    "## Summary of `trim_messages` in LangChain\n",
    "\n",
    "`trim_messages` is a utility that helps **manage the size of the message history** being sent to the LLM. It ensures that the **total token count** of the messages remains **within a specified limit**, like `max_tokens=200`. You can configure:\n",
    "- `strategy=\"last\"` â€“ Keep the most recent messages.\n",
    "- `include_system=True` â€“ Retain system prompts.\n",
    "- `allow_partial=False` â€“ Only complete messages are kept (no cutting in the middle).\n",
    "- `start_on=\"human\"` â€“ Decide from which type of message trimming should start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "# Trimmer for managing token limits\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=500,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb41dd",
   "metadata": {},
   "source": [
    "#### What is `RunnablePassthrough` in LangChain?\n",
    "\n",
    "`RunnablePassthrough` is a **utility runnable** in LangChain that:\n",
    "\n",
    "**Simply returns the input as-is** without modifying it.\n",
    "\n",
    "Think of it as a \"pass-through\" pipe â€” it doesn't change the data but allows you to **chain operations on the input** like `.assign(...)`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `RunnablePassthrough.assign(...)`\n",
    "\n",
    "This means:\n",
    "- Youâ€™re **starting with the full input dictionary**.\n",
    "- You're adding/modifying a field called `messages`.\n",
    "- You compute `messages` by:\n",
    "  ```python\n",
    "  itemgetter(\"messages\") | trimmer\n",
    "  ```\n",
    "  So:\n",
    "  - `itemgetter(\"messages\")`: extracts the `\"messages\"` key from the input.\n",
    "  - `trimmer`: trims that message list based on token constraints.\n",
    "  - `|` is LangChain's way of chaining operations.\n",
    "\n",
    "So you're **replacing/setting the `messages` field in the input with its trimmed version**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Without `RunnablePassthrough`?\n",
    "\n",
    "Youâ€™d have to handle this logic outside the chain or write custom wrappers â€” so it gives **clean composition and readability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4be0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the actual chain\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1f854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap chain with message history tracking\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb82156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnchantÃ©, Ravi !  \n",
      "\n",
      "Je suis ici pour vous aider. N'hÃ©sitez pas Ã  me poser toutes vos questions en franÃ§ais. Je ferai de mon mieux pour y rÃ©pondre. ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"My Name is Ravi\")],\"language\":\"french\"},\n",
    "    config=config\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5729fa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a fantastic goal, Ravi!  Software development is a great foundation for becoming an AI Engineer. \n",
      "\n",
      "What aspects of AI engineering are you most interested in?  \n",
      "\n",
      "Knowing your specific interests will help me give you more tailored advice.  For example, are you drawn to:\n",
      "\n",
      "* **Machine Learning?** (building algorithms that learn from data)\n",
      "* **Deep Learning?** (a subset of machine learning using artificial neural networks)\n",
      "* **Natural Language Processing?** (making computers understand and generate human language)\n",
      "* **Computer Vision?** ( enabling computers to \"see\" and interpret images)\n",
      "* **Robotics?** (designing and building intelligent robots)\n",
      "\n",
      "\n",
      "Tell me more about your aspirations, and let's explore the path to becoming an AI Engineer together!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"I am a software developer and aspire to become AI Engineer\")],\"language\":\"english\"},\n",
    "    config=config\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef63a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Ravi!  ðŸ˜Š  I remember that you told me at the beginning.  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Whats my name\")],\"language\":\"hindi\"},\n",
    "    config=config\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e6856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fefbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20abb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
