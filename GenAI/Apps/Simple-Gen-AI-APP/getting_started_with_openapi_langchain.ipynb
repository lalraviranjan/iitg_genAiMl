{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b095f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# will be used for langchain API calls and langchain tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['Langchain_Project'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3974e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000020A0D790EC0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020A0CD2A5D0> root_client=<openai.OpenAI object at 0x0000020A0D7E2FC0> root_async_client=<openai.AsyncOpenAI object at 0x0000020A0D790D10> model_name='gpt-4.1-nano' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "# ChatOpenAI is a wrapper around OpenAI's chat models, including gpt-3.5-turbo and gpt-4-turbo and others.\n",
    "# The model name is passed as a parameter to the constructor.\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano')\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d41947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a subset of artificial intelligence technologies designed to create, produce, or generate new content that resembles existing data. This content can include text, images, audio, video, or other data forms. Generative AI models learn patterns and structures from large datasets and then use this knowledge to produce novel outputs that are similar to the training data.\\n\\nCommon types of generative AI models include:\\n\\n- **Generative Adversarial Networks (GANs):** Consist of two neural networks—the generator and the discriminator—that compete to produce realistic synthetic data, such as realistic images or videos.\\n- **Variational Autoencoders (VAEs):** Used to generate new data similar to the input data by learning efficient representations.\\n- **Transformers (e.g., GPT series):** Designed primarily for generating coherent and contextually relevant text, but also adapted for other modalities like images and audio.\\n\\nApplications of generative AI include content creation, data augmentation, virtual assistants, art and music generation, deepfakes, and more. It has the potential to revolutionize various industries by automating creative tasks and enabling new forms of digital expression.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 13, 'total_tokens': 243, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_c1fb89028d', 'id': 'chatcmpl-BO1t00ikEnRxKZESsYEjMfO5SDmto', 'finish_reason': 'stop', 'logprobs': None} id='run-e66c30ea-b186-4635-bfc2-c343daafa65c-0' usage_metadata={'input_tokens': 13, 'output_tokens': 230, 'total_tokens': 243, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# The invoke() method is a more advanced way to call the model, allowing for more control over the input and output.\n",
    "result = llm.invoke(\"What is Generative AI?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66740e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a subset of artificial intelligence that uses machine learning models to generate data similar to the ones used to train it.\n",
      "It can create anything from music, images, text or even speech.\n",
      "Examples of Generative AI technologies include deepfake techniques, text auto-completion software, and predictive text algorithms.\n",
      "It works by understanding the patterns and structures within the data, learning from it, and producing new, original content that mimics the structure of the original data.\n"
     ]
    }
   ],
   "source": [
    "# Split the content by full stops and print each sentence on a new line\n",
    "sentences = result.content.split(\".\")\n",
    "for sentence in sentences:\n",
    "    if sentence.strip():\n",
    "        print(sentence.strip() + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca4c1a",
   "metadata": {},
   "source": [
    "### What is a ChatPrompt Template?\n",
    "\n",
    "A **ChatPrompt Template** is a structured way to define and format prompts for conversational AI models, such as OpenAI's GPT models. It allows you to create reusable and dynamic templates for generating prompts, which can be customized with specific inputs or variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Features:\n",
    "1. **Dynamic Inputs**: You can define placeholders in the template that can be replaced with user-provided inputs at runtime.\n",
    "2. **Reusability**: Templates can be reused across multiple queries or workflows, ensuring consistency in prompt formatting.\n",
    "3. **Customizable**: You can design templates to suit specific tasks, such as question answering, summarization, or creative writing.\n",
    "4. **Integration**: ChatPrompt Templates are often used with libraries like LangChain to streamline interactions with LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Use Case:\n",
    "Suppose you want to create a prompt for a chatbot that answers questions about a specific topic. A ChatPrompt Template can help you define the structure of the conversation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Code:\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a ChatPrompt Template\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
    ")\n",
    "\n",
    "# Use the template with a specific input\n",
    "formatted_prompt = template.format(question=\"What is the capital of France?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1736df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Output:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13465e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are a helpful assistant. Answer the following question:\n",
    "\n",
    "What is the capital of France?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915d10d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Benefits:\n",
    "- **Consistency**: Ensures that prompts follow a consistent format, improving the quality of responses.\n",
    "- **Efficiency**: Reduces the effort required to manually create prompts for each query.\n",
    "- **Flexibility**: Allows you to adapt the template for different tasks or use cases.\n",
    "\n",
    "ChatPrompt Templates are a powerful tool for building robust and scalable applications with conversational AI models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd243215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatprompt Template\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# ChatPromptTemplate is a class that allows you to create a prompt template for chat models.\n",
    "# It takes a list of messages as input, where each message is a tuple containing the role (system, user, assistant) and the content.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466326cf",
   "metadata": {},
   "source": [
    "### What is a Chain?\n",
    "\n",
    "A **Chain** in LangChain is a sequence of operations or steps that are executed in a specific order to process inputs and generate outputs. Chains are used to combine multiple components, such as prompts, language models, and tools, into a cohesive workflow for solving complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Features:\n",
    "1. **Modularity**: Chains allow you to combine different components (e.g., prompts, LLMs, retrievers) into a single pipeline.\n",
    "2. **Reusability**: Chains can be reused across different workflows, making them efficient for repetitive tasks.\n",
    "3. **Customizable**: You can design chains to suit specific use cases, such as question answering, summarization, or multi-step reasoning.\n",
    "4. **Integration**: Chains can integrate with other LangChain components, such as retrievers, vector stores, and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0731c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a development and deployment platform designed by Langchain to facilitate the building, testing, and deploying large language models (LLMs) and AI-powered applications. It provides developers with tools to create custom AI workflows, manage model interactions, and streamline the integration of language models into various applications. Langsmith emphasizes ease of use, collaboration, and scalable deployment, allowing teams to experiment with prompt engineering, monitor model performance, and iterate efficiently.\\n\\nKey features of Langsmith include:\\n- **Interactive Development Environment:** Tools for crafting and refining prompts, testing models, and exploring different configurations.\\n- **Model Management:** Capabilities to manage multiple models, version control, and deployment.\\n- **Monitoring and Analytics:** Insights into model usage, response quality, and potential issues.\\n- **Seamless Integration:** APIs and SDKs to embed language models into existing applications or new workflows.\\n\\nOverall, Langsmith aims to simplify the lifecycle of large language model applications, making it easier for developers and organizations to leverage AI effectively.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 34, 'total_tokens': 236, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_20621fa8f3', 'id': 'chatcmpl-BO1tJ1NE5u4fcLeUThE2DVjjxlzrf', 'finish_reason': 'stop', 'logprobs': None} id='run-1c269c42-803e-4992-aac8-27c1f0934aad-0' usage_metadata={'input_tokens': 34, 'output_tokens': 202, 'total_tokens': 236, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Create a Chain with the prompt and LLM\n",
    "# prompt creates the input messages from your input (like {\"input\": \"Explain...\"})\n",
    "# llm consumes those messages and returns a response\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"input\": \"What is Langsmith by Langchain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea46c51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa25a49",
   "metadata": {},
   "source": [
    "### StrOutputParser \n",
    "is a parser that converts the output of an LLM (usually a ChatMessage or a dictionary) into a plain string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13cd6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! As of my knowledge cutoff in October 2023, LangSmith is a platform designed to facilitate the development, deployment, and management of language models and NLP applications. It provides tools for data annotation, model training, evaluation, and deployment, enabling developers to build more effective and scalable language-based solutions. LangSmith emphasizes user-friendly interfaces and integrations with popular machine learning frameworks, making it accessible for both researchers and enterprise users. If you're looking for specific features or use cases, please let me know!\n"
     ]
    }
   ],
   "source": [
    "# stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# here when we print the response, it will be a string instead of a LangChain object\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
