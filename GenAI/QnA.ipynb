{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4f8bf1",
   "metadata": {},
   "source": [
    "The two `ChatPromptTemplate` objects are quite similar, but they differ in how they are constructed and the structure of the prompt.\n",
    "\n",
    "### 1. **Using `from_messages`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant. Please respond to the question asked.\"),\n",
    "        (\"user\", \"Question: {question}\")\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_messages`** creates a prompt template using a list of message tuples.\n",
    "- Each tuple contains two elements:\n",
    "  1. **Role**: This defines the role of the speaker, such as `\"system\"` and `\"user\"`. These roles help the language model distinguish between who is speaking (the system or the user).\n",
    "  2. **Message**: This is the text of the message that will be used. It can contain placeholders (like `{question}`) which will be filled at runtime with actual data.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to have a more structured conversation flow with clearly defined roles (e.g., system, user). It's often used when different messages need to come from different roles.\n",
    "  \n",
    "  For example:\n",
    "  - The **system** message sets up the role of the assistant.\n",
    "  - The **user** message provides the context or question dynamically using the `{question}` placeholder.\n",
    "\n",
    "### 2. **Using `from_template`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_template`** creates a prompt template from a string template.\n",
    "- The template allows for more flexibility in how you structure the prompt, and you can include placeholders like `{context}` that will be replaced with actual data at runtime.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to define the structure of the prompt in a more flexible, free-form way.\n",
    "- Itâ€™s especially useful if you need to design complex prompt templates with multiple placeholders or specific formatting, such as showing the context to the model or asking a more detailed question based on provided information.\n",
    "\n",
    "In this example, the prompt asks the model to answer the question based on the provided context, and the `{context}` placeholder will be replaced with actual context information when the prompt is invoked.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Message Structure**:\n",
    "   - `from_messages`: You are explicitly defining the roles (e.g., system, user) and the specific message content.\n",
    "   - `from_template`: You are defining a full prompt template where placeholders are embedded into the structure of the prompt. Itâ€™s more flexible for different styles.\n",
    "\n",
    "2. **Use Case**:\n",
    "   - `from_messages` is more useful when you need a structured prompt with different roles, such as in a dialogue-based system.\n",
    "   - `from_template` is more suitable for free-form or specialized prompt designs, particularly when you have more complex templates with multiple placeholders.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - `from_messages`: More rigid structure (you define the roles and the messages).\n",
    "   - `from_template`: More flexible, as the whole template is treated as a string where you can include placeholders dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c1a01",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `{context}` placeholder will be dynamically replaced with the value of `context` when you invoke the prompt. It doesn't inherently \"remember\" the context, but when you provide it as an argument during the prompt invocation, it will be inserted into the prompt at runtime.\n",
    "\n",
    "#### Key Points:\n",
    "- **Dynamic Insertion**: The `{context}` placeholder is replaced with actual content (usually provided by you in the invocation of the prompt). For example, the context might be a document or text chunk containing information that the LLM will use to answer the question.\n",
    "- **One-Time Context**: The context is provided on a per-request basis. It doesn't get stored or remembered between different invocations. If you want to remember context across different queries, you will need to manually manage and feed that context back into the prompt.\n",
    "  \n",
    "  In a practical scenario, you can store and append context after each interaction, or if you are working with a document-based context, you might append relevant parts of it to the prompt every time the model is called.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "context = \"LangChain is a framework for building applications powered by LLMs.\"\n",
    "question = \"What is LangChain?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Here, the context is dynamically inserted into the template when you use it.\n",
    "final_prompt = prompt.format(context=context)\n",
    "print(final_prompt)\n",
    "```\n",
    "\n",
    "This would produce a prompt like:\n",
    "\n",
    "```\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "LangChain is a framework for building applications powered by LLMs.\n",
    "</context>\n",
    "What is LangChain?\n",
    "```\n",
    "\n",
    "Thus, **context is only available for that specific call** to the prompt, and you would need to handle the logic to keep track of context over multiple queries if needed (e.g., by appending previous context to new ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca36dc",
   "metadata": {},
   "source": [
    "### ðŸš€ What is LCEL?\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a *concise, chainable syntax* for composing LangChain components like prompts, models, retrievers, and output parsers in a pipeline. It simplifies building **modular**, **readable**, and **reusable** workflows for language model applications.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§± Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "- `prompt`: prepares the prompt\n",
    "- `llm`: runs the model (like OpenAI, Llama, Ollama, etc.)\n",
    "- `output_parser`: formats the model output (e.g., just text)\n",
    "\n",
    "This is LCEL in action: components connected using the pipe (`|`) operator like Unix pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Benefits of LCEL:\n",
    "\n",
    "1. **Composable**: Easily plug different components together.\n",
    "2. **Readable**: Clear, linear flow of data.\n",
    "3. **Testable**: Each component can be tested independently.\n",
    "4. **Reusable**: Swap out components (e.g., change LLMs or prompt templates) without rewriting logic.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Without LCEL vs With LCEL\n",
    "\n",
    "**Without LCEL:**\n",
    "```python\n",
    "prompt_output = prompt.format(question=\"What is LCEL?\")\n",
    "model_output = llm.invoke(prompt_output)\n",
    "response = output_parser.parse(model_output)\n",
    "```\n",
    "\n",
    "**With LCEL:**\n",
    "```python\n",
    "response = (prompt | llm | output_parser).invoke({\"question\": \"What is LCEL?\"})\n",
    "```\n",
    "\n",
    "Same result, but cleaner and more modular using LCEL.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒŸ Summary:\n",
    "\n",
    "**LCEL** makes LangChain development:\n",
    "- more elegant âœ¨\n",
    "- more composable ðŸ”§\n",
    "- and easier to maintain ðŸ“¦\n",
    "\n",
    "It's one of LangChain's most powerful features for chaining operations cleanly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fec0ce",
   "metadata": {},
   "source": [
    "### ðŸ§  What is **Groq**?\n",
    "\n",
    "**Groq** is a company that has built its own ultra-fast, low-latency AI inference engine â€” **not a model**, but the *hardware and software infrastructure* that runs LLMs and AI workloads **faster** than traditional GPUs and CPUs.\n",
    "\n",
    "Itâ€™s similar to how NVIDIA provides GPUs, but Groq created a new architecture designed specifically for AI inference.\n",
    "\n",
    "---\n",
    "\n",
    "### âš¡ What is the **Groq API**?\n",
    "\n",
    "The **Groq API** gives you access to **pre-loaded large language models** (LLMs) that run on Groqâ€™s custom hardware â€” specifically their **LPUâ„¢ (Language Processing Unit)** chips â€” through a simple, fast API.\n",
    "\n",
    "Right now, Groq offers models like:\n",
    "\n",
    "- **Mixtral 8x7B** (MoE â€” Mixture of Experts)\n",
    "- **Gemma**\n",
    "- **LLaMA models**\n",
    "\n",
    "They do **not train models** â€” they optimize how *existing open-source models* are **served at blazing speeds**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ What makes Groq special?\n",
    "\n",
    "1. **Insane speed**: Latency as low as **1 ms/token**. Thatâ€™s **faster than GPU or TPU inference**.\n",
    "2. **Deterministic latency**: You get consistent response times.\n",
    "3. **Built for inference only**: Optimized for serving, not training.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© What is the **LPU AI interface engine**?\n",
    "\n",
    "The **LPU (Language Processing Unit)** is Groqâ€™s **custom chip architecture** â€” itâ€™s like their own version of a GPU but optimized **only for AI inference** â€” especially for **language models**.\n",
    "\n",
    "#### LPU Interface Engine highlights:\n",
    "\n",
    "- Designed to process **billions of tokens per second**\n",
    "- Efficient for **batch and real-time inference**\n",
    "- Runs on their **GroqNode servers**\n",
    "- Interfaces with API or LangChain, etc.\n",
    "\n",
    "You can think of it like this:\n",
    "> **LPU = AI Supercharger for LLMs**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”— Use cases for Groq:\n",
    "\n",
    "- **Chatbots** that require super low-latency\n",
    "- **RAG applications** with large context windows\n",
    "- **Streaming responses** for UX like ChatGPT\n",
    "- Any **high-throughput inference** scenario\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Summary\n",
    "\n",
    "| Feature               | Groq |\n",
    "|-----------------------|------|\n",
    "| **Type**              | Hardware + AI Inference Engine |\n",
    "| **API**               | Serves ultra-fast open-source LLMs |\n",
    "| **Special Chip**      | LPU (Language Processing Unit) |\n",
    "| **Use Case**          | Blazing-fast inference (chatbots, assistants, RAG) |\n",
    "| **Model Ownership**   | Runs open-source models (not owned/trained by Groq) |\n",
    "\n",
    "---\n",
    "\n",
    "If you're building something with **LangChain**, **RAG**, or want to serve a chatbot **without GPU cost/latency**, Groq is a strong option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0933a",
   "metadata": {},
   "source": [
    "# What is Langserve\n",
    "**LangServe** is a deployment tool developed by the LangChain team to simplify serving LangChain applications as RESTful APIs. It allows developers to expose their LangChain chains, agents, or runnables over HTTP endpoints without writing extensive boilerplate code. Built on top of FastAPI and utilizing Pydantic for data validation, LangServe streamlines the process of transitioning from development to production.îˆ†\n",
    "\n",
    "### ðŸ”§ Key Features of LangServe\n",
    "\n",
    "- **Easy Deployment**:îˆƒQuickly deploy LangChain components as REST APIs with minimal setupîˆ„îˆ†\n",
    "\n",
    "- **Automatic API Documentation**:îˆƒGenerates interactive API docs using Swagger and JSONSchema, facilitating easier testing and integrationîˆ„îˆ†\n",
    "\n",
    "- **Efficient Endpoints**:îˆƒProvides `/invoke`, `/batch`, and `/stream` endpoints to handle various request types efficientlyîˆ„îˆ†\n",
    "\n",
    "- **Streaming Logs**:îˆƒOffers a `/stream_log` endpoint to stream intermediate steps from your chain or agent, aiding in debugging and monitoringîˆ„îˆ†\n",
    "\n",
    "- **Client Integration**:îˆƒIncludes a JavaScript client (LangChain.js) to interact with deployed LangServe routes, enabling seamless frontend integrationîˆ„îˆ†\n",
    "\n",
    "### ðŸš€ Getting Started with LangServe\n",
    "\n",
    "1. **Installation**:\n",
    "\n",
    "   Install LangChain and LangServe using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install langchain langserve\n",
    "   ```\n",
    "\n",
    "2. **Define Your Chain**:\n",
    "\n",
    "   Create your LangChain chain, agent, or runnable as you normally would.\n",
    "\n",
    "3. **Deploy with LangServe**:\n",
    "\n",
    "   Use LangServe to expose your chain as a REST API:\n",
    "\n",
    "   ```python\n",
    "   from langserve import add_routes\n",
    "   from fastapi import FastAPI\n",
    "\n",
    "   app = FastAPI()\n",
    "   add_routes(app, your_chain, path=\"/your-endpoint\")\n",
    "   ```\n",
    "\n",
    "4. **Run the Server**:\n",
    "\n",
    "   Start your FastAPI server to serve the API:\n",
    "\n",
    "   ```bash\n",
    "   uvicorn your_app:app --reload\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
