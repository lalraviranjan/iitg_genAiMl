{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4f8bf1",
   "metadata": {},
   "source": [
    "The two `ChatPromptTemplate` objects are quite similar, but they differ in how they are constructed and the structure of the prompt.\n",
    "\n",
    "### 1. **Using `from_messages`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant. Please respond to the question asked.\"),\n",
    "        (\"user\", \"Question: {question}\")\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_messages`** creates a prompt template using a list of message tuples.\n",
    "- Each tuple contains two elements:\n",
    "  1. **Role**: This defines the role of the speaker, such as `\"system\"` and `\"user\"`. These roles help the language model distinguish between who is speaking (the system or the user).\n",
    "  2. **Message**: This is the text of the message that will be used. It can contain placeholders (like `{question}`) which will be filled at runtime with actual data.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to have a more structured conversation flow with clearly defined roles (e.g., system, user). It's often used when different messages need to come from different roles.\n",
    "  \n",
    "  For example:\n",
    "  - The **system** message sets up the role of the assistant.\n",
    "  - The **user** message provides the context or question dynamically using the `{question}` placeholder.\n",
    "\n",
    "### 2. **Using `from_template`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_template`** creates a prompt template from a string template.\n",
    "- The template allows for more flexibility in how you structure the prompt, and you can include placeholders like `{context}` that will be replaced with actual data at runtime.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to define the structure of the prompt in a more flexible, free-form way.\n",
    "- It‚Äôs especially useful if you need to design complex prompt templates with multiple placeholders or specific formatting, such as showing the context to the model or asking a more detailed question based on provided information.\n",
    "\n",
    "In this example, the prompt asks the model to answer the question based on the provided context, and the `{context}` placeholder will be replaced with actual context information when the prompt is invoked.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Message Structure**:\n",
    "   - `from_messages`: You are explicitly defining the roles (e.g., system, user) and the specific message content.\n",
    "   - `from_template`: You are defining a full prompt template where placeholders are embedded into the structure of the prompt. It‚Äôs more flexible for different styles.\n",
    "\n",
    "2. **Use Case**:\n",
    "   - `from_messages` is more useful when you need a structured prompt with different roles, such as in a dialogue-based system.\n",
    "   - `from_template` is more suitable for free-form or specialized prompt designs, particularly when you have more complex templates with multiple placeholders.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - `from_messages`: More rigid structure (you define the roles and the messages).\n",
    "   - `from_template`: More flexible, as the whole template is treated as a string where you can include placeholders dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c1a01",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `{context}` placeholder will be dynamically replaced with the value of `context` when you invoke the prompt. It doesn't inherently \"remember\" the context, but when you provide it as an argument during the prompt invocation, it will be inserted into the prompt at runtime.\n",
    "\n",
    "#### Key Points:\n",
    "- **Dynamic Insertion**: The `{context}` placeholder is replaced with actual content (usually provided by you in the invocation of the prompt). For example, the context might be a document or text chunk containing information that the LLM will use to answer the question.\n",
    "- **One-Time Context**: The context is provided on a per-request basis. It doesn't get stored or remembered between different invocations. If you want to remember context across different queries, you will need to manually manage and feed that context back into the prompt.\n",
    "  \n",
    "  In a practical scenario, you can store and append context after each interaction, or if you are working with a document-based context, you might append relevant parts of it to the prompt every time the model is called.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "context = \"LangChain is a framework for building applications powered by LLMs.\"\n",
    "question = \"What is LangChain?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Here, the context is dynamically inserted into the template when you use it.\n",
    "final_prompt = prompt.format(context=context)\n",
    "print(final_prompt)\n",
    "```\n",
    "\n",
    "This would produce a prompt like:\n",
    "\n",
    "```\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "LangChain is a framework for building applications powered by LLMs.\n",
    "</context>\n",
    "What is LangChain?\n",
    "```\n",
    "\n",
    "Thus, **context is only available for that specific call** to the prompt, and you would need to handle the logic to keep track of context over multiple queries if needed (e.g., by appending previous context to new ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca36dc",
   "metadata": {},
   "source": [
    "### üöÄ What is LCEL?\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a *concise, chainable syntax* for composing LangChain components like prompts, models, retrievers, and output parsers in a pipeline. It simplifies building **modular**, **readable**, and **reusable** workflows for language model applications.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "- `prompt`: prepares the prompt\n",
    "- `llm`: runs the model (like OpenAI, Llama, Ollama, etc.)\n",
    "- `output_parser`: formats the model output (e.g., just text)\n",
    "\n",
    "This is LCEL in action: components connected using the pipe (`|`) operator like Unix pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Benefits of LCEL:\n",
    "\n",
    "1. **Composable**: Easily plug different components together.\n",
    "2. **Readable**: Clear, linear flow of data.\n",
    "3. **Testable**: Each component can be tested independently.\n",
    "4. **Reusable**: Swap out components (e.g., change LLMs or prompt templates) without rewriting logic.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Without LCEL vs With LCEL\n",
    "\n",
    "**Without LCEL:**\n",
    "```python\n",
    "prompt_output = prompt.format(question=\"What is LCEL?\")\n",
    "model_output = llm.invoke(prompt_output)\n",
    "response = output_parser.parse(model_output)\n",
    "```\n",
    "\n",
    "**With LCEL:**\n",
    "```python\n",
    "response = (prompt | llm | output_parser).invoke({\"question\": \"What is LCEL?\"})\n",
    "```\n",
    "\n",
    "Same result, but cleaner and more modular using LCEL.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Summary:\n",
    "\n",
    "**LCEL** makes LangChain development:\n",
    "- more elegant ‚ú®\n",
    "- more composable üîß\n",
    "- and easier to maintain üì¶\n",
    "\n",
    "It's one of LangChain's most powerful features for chaining operations cleanly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fec0ce",
   "metadata": {},
   "source": [
    "### üß† What is **Groq**?\n",
    "\n",
    "**Groq** is a company that has built its own ultra-fast, low-latency AI inference engine ‚Äî **not a model**, but the *hardware and software infrastructure* that runs LLMs and AI workloads **faster** than traditional GPUs and CPUs.\n",
    "\n",
    "It‚Äôs similar to how NVIDIA provides GPUs, but Groq created a new architecture designed specifically for AI inference.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° What is the **Groq API**?\n",
    "\n",
    "The **Groq API** gives you access to **pre-loaded large language models** (LLMs) that run on Groq‚Äôs custom hardware ‚Äî specifically their **LPU‚Ñ¢ (Language Processing Unit)** chips ‚Äî through a simple, fast API.\n",
    "\n",
    "Right now, Groq offers models like:\n",
    "\n",
    "- **Mixtral 8x7B** (MoE ‚Äî Mixture of Experts)\n",
    "- **Gemma**\n",
    "- **LLaMA models**\n",
    "\n",
    "They do **not train models** ‚Äî they optimize how *existing open-source models* are **served at blazing speeds**.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ What makes Groq special?\n",
    "\n",
    "1. **Insane speed**: Latency as low as **1 ms/token**. That‚Äôs **faster than GPU or TPU inference**.\n",
    "2. **Deterministic latency**: You get consistent response times.\n",
    "3. **Built for inference only**: Optimized for serving, not training.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© What is the **LPU AI interface engine**?\n",
    "\n",
    "The **LPU (Language Processing Unit)** is Groq‚Äôs **custom chip architecture** ‚Äî it‚Äôs like their own version of a GPU but optimized **only for AI inference** ‚Äî especially for **language models**.\n",
    "\n",
    "#### LPU Interface Engine highlights:\n",
    "\n",
    "- Designed to process **billions of tokens per second**\n",
    "- Efficient for **batch and real-time inference**\n",
    "- Runs on their **GroqNode servers**\n",
    "- Interfaces with API or LangChain, etc.\n",
    "\n",
    "You can think of it like this:\n",
    "> **LPU = AI Supercharger for LLMs**\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Use cases for Groq:\n",
    "\n",
    "- **Chatbots** that require super low-latency\n",
    "- **RAG applications** with large context windows\n",
    "- **Streaming responses** for UX like ChatGPT\n",
    "- Any **high-throughput inference** scenario\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Summary\n",
    "\n",
    "| Feature               | Groq |\n",
    "|-----------------------|------|\n",
    "| **Type**              | Hardware + AI Inference Engine |\n",
    "| **API**               | Serves ultra-fast open-source LLMs |\n",
    "| **Special Chip**      | LPU (Language Processing Unit) |\n",
    "| **Use Case**          | Blazing-fast inference (chatbots, assistants, RAG) |\n",
    "| **Model Ownership**   | Runs open-source models (not owned/trained by Groq) |\n",
    "\n",
    "---\n",
    "\n",
    "If you're building something with **LangChain**, **RAG**, or want to serve a chatbot **without GPU cost/latency**, Groq is a strong option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0933a",
   "metadata": {},
   "source": [
    "# What is Langserve\n",
    "**LangServe** is a deployment tool developed by the LangChain team to simplify serving LangChain applications as RESTful APIs. It allows developers to expose their LangChain chains, agents, or runnables over HTTP endpoints without writing extensive boilerplate code. Built on top of FastAPI and utilizing Pydantic for data validation, LangServe streamlines the process of transitioning from development to production.ÓàÜ\n",
    "\n",
    "### üîß Key Features of LangServe\n",
    "\n",
    "- **Easy Deployment**:ÓàÉQuickly deploy LangChain components as REST APIs with minimal setupÓàÑÓàÜ\n",
    "\n",
    "- **Automatic API Documentation**:ÓàÉGenerates interactive API docs using Swagger and JSONSchema, facilitating easier testing and integrationÓàÑÓàÜ\n",
    "\n",
    "- **Efficient Endpoints**:ÓàÉProvides `/invoke`, `/batch`, and `/stream` endpoints to handle various request types efficientlyÓàÑÓàÜ\n",
    "\n",
    "- **Streaming Logs**:ÓàÉOffers a `/stream_log` endpoint to stream intermediate steps from your chain or agent, aiding in debugging and monitoringÓàÑÓàÜ\n",
    "\n",
    "- **Client Integration**:ÓàÉIncludes a JavaScript client (LangChain.js) to interact with deployed LangServe routes, enabling seamless frontend integrationÓàÑÓàÜ\n",
    "\n",
    "### üöÄ Getting Started with LangServe\n",
    "\n",
    "1. **Installation**:\n",
    "\n",
    "   Install LangChain and LangServe using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install langchain langserve\n",
    "   ```\n",
    "\n",
    "2. **Define Your Chain**:\n",
    "\n",
    "   Create your LangChain chain, agent, or runnable as you normally would.\n",
    "\n",
    "3. **Deploy with LangServe**:\n",
    "\n",
    "   Use LangServe to expose your chain as a REST API:\n",
    "\n",
    "   ```python\n",
    "   from langserve import add_routes\n",
    "   from fastapi import FastAPI\n",
    "\n",
    "   app = FastAPI()\n",
    "   add_routes(app, your_chain, path=\"/your-endpoint\")\n",
    "   ```\n",
    "\n",
    "4. **Run the Server**:\n",
    "\n",
    "   Start your FastAPI server to serve the API:\n",
    "\n",
    "   ```bash\n",
    "   uvicorn your_app:app --reload\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa495c1",
   "metadata": {},
   "source": [
    "#### What is **Message History** in an AI Chatbot?\n",
    "\n",
    "In the context of AI chatbots, **message history** refers to the **persistent or temporary memory of the past exchanges** between the user and the chatbot ‚Äî including both the user's inputs and the model's outputs.\n",
    "\n",
    "This **conversation history** helps make the chatbot **stateful**, meaning it can **\"remember\" previous messages** and **respond in a more coherent and context-aware way**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ Why is Message History Important?\n",
    "\n",
    "Most LLMs (like GPT, Claude, etc.) are **stateless by default**, which means:\n",
    "- Every prompt is treated **independently**, unless you **manually provide context**.\n",
    "- Without message history, the bot would \"forget\" what was said earlier in the conversation.\n",
    "\n",
    "So to **maintain context**, we pass prior messages along with the new user message in the prompt to the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Example Use Case\n",
    "\n",
    "#### Without Message History:\n",
    "```plaintext\n",
    "User: What's the weather in Delhi?\n",
    "Bot: It's 34¬∞C and sunny in Delhi.\n",
    "\n",
    "User: What about tomorrow?\n",
    "Bot: (Without context, it doesn't know you're still talking about Delhi)\n",
    "```\n",
    "\n",
    "#### With Message History:\n",
    "```plaintext\n",
    "User: What's the weather in Delhi?\n",
    "Bot: It's 34¬∞C and sunny in Delhi.\n",
    "\n",
    "User: What about tomorrow?\n",
    "Bot: Tomorrow in Delhi, it's expected to be 36¬∞C with some clouds.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Optional: Store Message History in External Datastores\n",
    "\n",
    "You can persist the message history in:\n",
    "- **In-memory (short sessions)**\n",
    "- **Vector Stores (semantic search)**\n",
    "- **Databases (structured storage)**\n",
    "- **Redis (fast key-value store)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e443d",
   "metadata": {},
   "source": [
    "## üîπ `ChatMessageHistory` (from `langchain_community.chat_message_histories`)\n",
    "This is a **concrete implementation** of a message history class that stores messages **in memory** for a given session.\n",
    "\n",
    "#### üîç Purpose:\n",
    "- To **store and retrieve chat messages** (e.g., Human and AI messages) for a session.\n",
    "- Acts like a log of conversations.\n",
    "\n",
    "#### üí° Used When:\n",
    "You want to **track conversation history** **in RAM** (good for quick testing or non-persistent memory).\n",
    "\n",
    "#### üìå Example:\n",
    "```python\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hi\")\n",
    "history.add_ai_message(\"Hello, how can I help?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ `BaseChatMessageHistory` (from `langchain_core.chat_history`)\n",
    "This is an **abstract base class** (interface) that all chat message history implementations must inherit from.\n",
    "\n",
    "#### üîç Purpose:\n",
    "- Defines the **expected methods** like `add_user_message()`, `add_ai_message()`, `messages` (getter).\n",
    "- Used so LangChain can accept any history backend (memory, Redis, DB, etc.) as long as it conforms to this interface.\n",
    "\n",
    "#### ‚úÖ You don‚Äôt instantiate this. You **inherit** from it to create your own history class.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ `RunnableWithMessageHistory` (from `langchain_core.runnables.history`)\n",
    "This is a **wrapper around a chain or model** to add **memory support** (i.e., chat history state tracking).\n",
    "\n",
    "#### üîç Purpose:\n",
    "- Wraps any `Runnable` (like an LLM, chain, or tool).\n",
    "- Injects **past messages** from the message history into the chain's input before invocation.\n",
    "- Automatically **appends new inputs/outputs** into the history after invocation.\n",
    "\n",
    "#### ‚öôÔ∏è Example:\n",
    "```python\n",
    "wrapped = RunnableWithMessageHistory(my_llm_chain, get_session_history)\n",
    "response = wrapped.invoke([HumanMessage(content=\"Tell me a joke\")], config={\"configurable\": {\"session_id\": \"abc\"}})\n",
    "```\n",
    "\n",
    "üß† Internally:\n",
    "- Before the call, it **loads messages** using the session ID.\n",
    "- It **appends your new user input** to the message list.\n",
    "- Then, **calls the chain/model** with the full message history.\n",
    "- Finally, **stores the model's output** back to the history store.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "| Class | Role | Scope |\n",
    "|-------|------|-------|\n",
    "| `ChatMessageHistory` | Stores messages in RAM for a session | Concrete implementation |\n",
    "| `BaseChatMessageHistory` | Abstract interface for history | Foundation for all message memory |\n",
    "| `RunnableWithMessageHistory` | Adds stateful memory to any chain/model | Wraps logic and history together |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359f255",
   "metadata": {},
   "source": [
    "## **Difference between `HumanMessage` and `ChatPromptTemplate` invocation**, how they work, and when to use each.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. **Using `HumanMessage` for invocation**\n",
    "\n",
    "### ‚úÖ What it looks like:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([\n",
    "    HumanMessage(content=\"What's my name?\")\n",
    "])\n",
    "```\n",
    "\n",
    "### üîç How it works:\n",
    "\n",
    "- You‚Äôre directly sending structured chat messages (e.g., `HumanMessage`, `AIMessage`, `SystemMessage`) to the model.\n",
    "- This is **low-level** control ‚Äî great when you're using something like `RunnableWithMessageHistory` that **automatically manages history**.\n",
    "- No templating or formatting required.\n",
    "\n",
    "### ‚úÖ When to use:\n",
    "\n",
    "- When you're using memory (`RunnableWithMessageHistory`) and don‚Äôt need to format prompts yourself.\n",
    "- In simple stateful bots where message roles are more important than prompt templates.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. **Using `ChatPromptTemplate` for invocation**\n",
    "\n",
    "### ‚úÖ What it looks like:\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"question\": \"What's my name?\"})\n",
    "```\n",
    "\n",
    "### üîç How it works:\n",
    "\n",
    "- You define a **template for a conversation**, using placeholders (e.g., `{question}`).\n",
    "- It gives you **full control over how prompts are structured** ‚Äî useful for:\n",
    "  - RAG (including context)\n",
    "  - Custom chat flow\n",
    "  - Advanced agent orchestration\n",
    "\n",
    "### ‚úÖ When to use:\n",
    "\n",
    "- When you need **fine-tuned prompts**.\n",
    "- When you integrate external context (like vector DB results).\n",
    "- When building **agentic pipelines**.\n",
    "- When system message plays a key role.\n",
    "\n",
    "---\n",
    "\n",
    "## ü•á Which is the **best technique**?\n",
    "\n",
    "| Use Case | Best Choice |\n",
    "|----------|-------------|\n",
    "| Simple stateful chatbot | `HumanMessage` with memory (`RunnableWithMessageHistory`) |\n",
    "| Custom instructions, prompt logic, RAG | `ChatPromptTemplate` |\n",
    "| Building agents or multi-step chains | `ChatPromptTemplate` |\n",
    "| Quick prototyping | `HumanMessage` |\n",
    "\n",
    "### üîî Rule of Thumb:\n",
    "\n",
    "- **Need memory & simplicity?** ‚Üí `HumanMessage` + memory wrapper.\n",
    "- **Need precision & flexibility?** ‚Üí `ChatPromptTemplate`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d2a33",
   "metadata": {},
   "source": [
    "Here is the updated notebook with all markdown cells cleaned up, and icons like ‚úÖ and others removed from the headers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f345f6",
   "metadata": {},
   "source": [
    "## **What is \"Context\" in an LLM?**\n",
    "\n",
    "**Context** in a Large Language Model (LLM) refers to:\n",
    "\n",
    "> ‚úÖ **The total amount of text (tokens) that the model can \"see\" or \"remember\" in a single interaction.**\n",
    "\n",
    "This includes:\n",
    "- The **current prompt** (user input + any system instructions)\n",
    "- **Prior conversation history** (if any)\n",
    "- The model‚Äôs **own responses**, if they're reused as context\n",
    "- Any other **background knowledge** passed in the prompt (like retrieved documents in RAG)\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Example: ChatGPT Context Window\n",
    "\n",
    "| Model | Context Window |\n",
    "|-------|----------------|\n",
    "| GPT-3.5 | 4K tokens |\n",
    "| GPT-4 (8K variant) | 8K tokens |\n",
    "| GPT-4 Turbo | 128K tokens! |\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Tokens ‚â† Words  \n",
    "LLMs don‚Äôt measure input by characters or words ‚Äî they use **tokens**.\n",
    "\n",
    "üß† Rule of Thumb:\n",
    "> üîπ 1 token ‚âà 4 characters of English text  \n",
    "> üîπ 1 token ‚âà ¬æ of a word (for English)\n",
    "\n",
    "So:  \n",
    "**\"ChatGPT is awesome!\"** ‚âà 5 tokens\n",
    "\n",
    "---\n",
    "\n",
    "### üö´ What Happens if You Exceed the Context Window?\n",
    "\n",
    "If your input (history + prompt + system + retrieved docs) is **too long**, the model:\n",
    "- Will **fail to process the prompt**, or\n",
    "- Will **drop earlier parts of the conversation**, which may lead to hallucination or irrelevant answers\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Why Context Matters\n",
    "\n",
    "- LLMs **don‚Äôt have memory** by default (like a brain).  \n",
    "- They can only **\"think\" based on what you give them right now**.\n",
    "- Managing context = managing what the model can \"remember\".\n",
    "\n",
    "That‚Äôs why tools like:\n",
    "- **Message history**\n",
    "- **Trimmers**\n",
    "- **Summarization chains**\n",
    "- **RAG (retrieval augmented generation)**  \n",
    "...are used to **efficiently feed context** to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b869b3",
   "metadata": {},
   "source": [
    "## `trim_messages` in LangChain\n",
    "\n",
    "`trim_messages` is a utility that helps **manage the size of the message history** being sent to the LLM. It ensures that the **total token count** of the messages remains **within a specified limit**, like `max_tokens=200`. You can configure:\n",
    "- `strategy=\"last\"` ‚Äì Keep the most recent messages.\n",
    "- `include_system=True` ‚Äì Retain system prompts.\n",
    "- `allow_partial=False` ‚Äì Only complete messages are kept (no cutting in the middle).\n",
    "- `start_on=\"human\"` ‚Äì Decide from which type of message trimming should start.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Benefits of `trim_messages`\n",
    "\n",
    "| üîπ **Benefit** | üîç **Explanation** |\n",
    "|---------------|--------------------|\n",
    "| üöÄ **Avoids context overflow** | Keeps message history within the model's context limit. Prevents errors or degraded model performance. |\n",
    "| üìè **Token control** | Manages token usage per prompt, helping stay within API or memory limits (e.g., OpenAI GPT-4‚Äôs 8K/32K). |\n",
    "| üß† **Keeps relevant history** | Uses strategies like \"keep last\" to retain only **recent and relevant** context for better coherence. |\n",
    "| üßº **Improves model output** | Helps model focus on the most recent context, avoiding dilution with outdated or irrelevant history. |\n",
    "| üí∞ **Reduces cost** | Fewer tokens passed = Lower API cost on usage-based billing models like OpenAI, Anthropic, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Should You Use `trim_messages`?\n",
    "\n",
    "**Yes, you absolutely should.** Especially if:\n",
    "- You‚Äôre building a **multi-turn chatbot** or assistant.\n",
    "- You're using **memory via RunnableWithMessageHistory**.\n",
    "- Your application may involve **long user sessions**.\n",
    "- You want **predictable token usage** and **performance stability**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Without `trim_messages`:\n",
    "Your chatbot can grow a giant context, eventually hitting model limits and causing:\n",
    "- Errors (`context_length_exceeded`)\n",
    "- Incomplete responses\n",
    "- Model hallucinations\n",
    "- Cost inefficiency\n",
    "\n",
    "---\n",
    "\n",
    "So yes ‚Äî `trim_messages` is **essential** for production-grade chatbot memory management. It's like garbage collection for your message history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a630f",
   "metadata": {},
   "source": [
    "## üîß What is a `Runnable` in LangChain?\n",
    "\n",
    "> A `Runnable` is **any object that can be invoked like a function** (using `.invoke()` or `|`) in a LangChain pipeline.\n",
    "\n",
    "Think of it as a standardized interface that says:\n",
    "\n",
    "> ‚ÄúHey, I know how to accept some input, process it, and return an output.‚Äù\n",
    "\n",
    "It could be:\n",
    "- An LLM\n",
    "- A prompt template\n",
    "- A retriever\n",
    "- A memory wrapper\n",
    "- A custom Python function\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Basic Capabilities of a `Runnable`\n",
    "\n",
    "All `Runnable`s implement methods like:\n",
    "\n",
    "| Method             | What it does                                          |\n",
    "|--------------------|-------------------------------------------------------|\n",
    "| `.invoke(input)`   | Synchronously runs the logic and returns the output   |\n",
    "| `.ainvoke(input)`  | Asynchronously runs the logic                         |\n",
    "| `.batch(inputs)`   | Runs on a list of inputs                              |\n",
    "| `.stream(input)`   | Streams tokens/output (e.g., from LLM)                |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Custom Runnable that doubles a number\n",
    "double = RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "print(double.invoke(5))  # Output: 10\n",
    "```\n",
    "\n",
    "This is now a component you can plug into a chain:\n",
    "\n",
    "```python\n",
    "pipeline = double | some_other_step | llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß¨ Real Power: **Composable Chains**\n",
    "\n",
    "Because everything is a Runnable:\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "All of those components (prompt, llm, parser) are `Runnable`s ‚Äî chained together seamlessly using the pipe (`|`) operator.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why It Matters\n",
    "\n",
    "The `Runnable` concept is what **unifies** LangChain:\n",
    "- You can treat LLMs, prompts, retrievers, memory, functions all the same.\n",
    "- You get a **flexible and readable pipeline** (like UNIX pipes or functional chaining).\n",
    "- Easier debugging, testing, and modularity.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "A **Runnable** in LangChain is:\n",
    "- Any component that knows how to accept input and return output.\n",
    "- A core building block in pipelines.\n",
    "- Supports both sync and async workflows.\n",
    "- Makes LangChain chains powerful, composable, and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab8b4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97eb8e20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5250501a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16e154f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
