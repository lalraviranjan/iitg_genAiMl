{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4f8bf1",
   "metadata": {},
   "source": [
    "The two `ChatPromptTemplate` objects are quite similar, but they differ in how they are constructed and the structure of the prompt.\n",
    "\n",
    "### 1. **Using `from_messages`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant. Please respond to the question asked.\"),\n",
    "        (\"user\", \"Question: {question}\")\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_messages`** creates a prompt template using a list of message tuples.\n",
    "- Each tuple contains two elements:\n",
    "  1. **Role**: This defines the role of the speaker, such as `\"system\"` and `\"user\"`. These roles help the language model distinguish between who is speaking (the system or the user).\n",
    "  2. **Message**: This is the text of the message that will be used. It can contain placeholders (like `{question}`) which will be filled at runtime with actual data.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to have a more structured conversation flow with clearly defined roles (e.g., system, user). It's often used when different messages need to come from different roles.\n",
    "  \n",
    "  For example:\n",
    "  - The **system** message sets up the role of the assistant.\n",
    "  - The **user** message provides the context or question dynamically using the `{question}` placeholder.\n",
    "\n",
    "### 2. **Using `from_template`**:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### How it works:\n",
    "- **`from_template`** creates a prompt template from a string template.\n",
    "- The template allows for more flexibility in how you structure the prompt, and you can include placeholders like `{context}` that will be replaced with actual data at runtime.\n",
    "\n",
    "#### Use Case:\n",
    "- This approach is useful when you want to define the structure of the prompt in a more flexible, free-form way.\n",
    "- It’s especially useful if you need to design complex prompt templates with multiple placeholders or specific formatting, such as showing the context to the model or asking a more detailed question based on provided information.\n",
    "\n",
    "In this example, the prompt asks the model to answer the question based on the provided context, and the `{context}` placeholder will be replaced with actual context information when the prompt is invoked.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Message Structure**:\n",
    "   - `from_messages`: You are explicitly defining the roles (e.g., system, user) and the specific message content.\n",
    "   - `from_template`: You are defining a full prompt template where placeholders are embedded into the structure of the prompt. It’s more flexible for different styles.\n",
    "\n",
    "2. **Use Case**:\n",
    "   - `from_messages` is more useful when you need a structured prompt with different roles, such as in a dialogue-based system.\n",
    "   - `from_template` is more suitable for free-form or specialized prompt designs, particularly when you have more complex templates with multiple placeholders.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - `from_messages`: More rigid structure (you define the roles and the messages).\n",
    "   - `from_template`: More flexible, as the whole template is treated as a string where you can include placeholders dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c1a01",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "The `{context}` placeholder will be dynamically replaced with the value of `context` when you invoke the prompt. It doesn't inherently \"remember\" the context, but when you provide it as an argument during the prompt invocation, it will be inserted into the prompt at runtime.\n",
    "\n",
    "#### Key Points:\n",
    "- **Dynamic Insertion**: The `{context}` placeholder is replaced with actual content (usually provided by you in the invocation of the prompt). For example, the context might be a document or text chunk containing information that the LLM will use to answer the question.\n",
    "- **One-Time Context**: The context is provided on a per-request basis. It doesn't get stored or remembered between different invocations. If you want to remember context across different queries, you will need to manually manage and feed that context back into the prompt.\n",
    "  \n",
    "  In a practical scenario, you can store and append context after each interaction, or if you are working with a document-based context, you might append relevant parts of it to the prompt every time the model is called.\n",
    "\n",
    "#### Example Usage:\n",
    "\n",
    "```python\n",
    "context = \"LangChain is a framework for building applications powered by LLMs.\"\n",
    "question = \"What is LangChain?\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Here, the context is dynamically inserted into the template when you use it.\n",
    "final_prompt = prompt.format(context=context)\n",
    "print(final_prompt)\n",
    "```\n",
    "\n",
    "This would produce a prompt like:\n",
    "\n",
    "```\n",
    "Answer the following question based only on provided context:\n",
    "<context>\n",
    "LangChain is a framework for building applications powered by LLMs.\n",
    "</context>\n",
    "What is LangChain?\n",
    "```\n",
    "\n",
    "Thus, **context is only available for that specific call** to the prompt, and you would need to handle the logic to keep track of context over multiple queries if needed (e.g., by appending previous context to new ones)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
