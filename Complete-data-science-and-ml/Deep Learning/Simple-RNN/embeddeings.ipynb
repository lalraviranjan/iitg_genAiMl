{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6910, 6898, 6426, 6741],\n",
       " [6910, 6898, 6426, 199],\n",
       " [6910, 506, 6426, 3282],\n",
       " [3052, 5528, 3769, 9942, 903],\n",
       " [3052, 5528, 3769, 9942, 3734],\n",
       " [8116, 6910, 9682, 6426, 9353],\n",
       " [749, 214, 1312, 9942]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### sentences\n",
    "sent=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "    'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]\n",
    "\n",
    "#define vocabulary size\n",
    "vocab_size=10000\n",
    "\n",
    "# one hot representation\n",
    "one_hot_rep = [one_hot(words, vocab_size) for words in sent]\n",
    "one_hot_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding representation  \n",
    "\n",
    "# The Embedding layer is used to convert integer-encoded words into dense vector representations of fixed size\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Import the pad_sequences function to ensure that input sequences have the same length \n",
    "# pads shorter sequences with zeros (or a specified value) and truncates longer sequences (making similar length of words in sentences)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 6910, 6898, 6426, 6741],\n",
       "       [   0,    0,    0,    0, 6910, 6898, 6426,  199],\n",
       "       [   0,    0,    0,    0, 6910,  506, 6426, 3282],\n",
       "       [   0,    0,    0, 3052, 5528, 3769, 9942,  903],\n",
       "       [   0,    0,    0, 3052, 5528, 3769, 9942, 3734],\n",
       "       [   0,    0,    0, 8116, 6910, 9682, 6426, 9353],\n",
       "       [   0,    0,    0,    0,  749,  214, 1312, 9942]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_length=8\n",
    "# pad the sequences to ensure uniform length\n",
    "# 0 is used for padding, if you use post padding, it will add 0s at the end of the sequence\n",
    "# if you use pre padding, it will add 0s at the beginning of the sequence\n",
    "# maxlen is the maximum length of the sequences\n",
    "embedded_docs=pad_sequences(one_hot_rep, padding='pre', maxlen=sent_length)\n",
    "embedded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How `model.add(Embedding(vocab_size, dim))` It Works During Training\n",
    "1. vocab_size:\n",
    "\n",
    "    - Defines the number of unique words in the vocabulary.\n",
    "    - Each word is assigned an integer index between 0 and vocab_size - 1.\n",
    "    - The Embedding layer creates an embedding matrix of shape (vocab_size, dim).\n",
    "    dim:\n",
    "\n",
    "2. dim:\n",
    "    - Specifies the size of the dense vector for each word.\n",
    "    - Each word index is mapped to a vector of size dim (e.g., [0.1, -0.2, 0.3, ...]).\n",
    "    - During Training:\n",
    "\n",
    "3. During training (model.fit())\n",
    "    - The embedding matrix is initialized randomly.\n",
    "    - When the model is trained using model.fit(), the embedding matrix is updated to learn meaningful - - word representations based on the task (e.g., sentiment analysis, text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature representation\n",
    "dim = 10\n",
    "# Create a Sequential model\n",
    "# The Sequential model is a linear stack of layers\n",
    "# The Embedding layer is the first layer in the model\n",
    "# The Embedding layer will learn the word embeddings during training\n",
    "# The Embedding layer takes the input sequences and converts them into dense vectors of fixed size\n",
    "model = Sequential()\n",
    "# Embedding layer\n",
    "# The first argument is the size of the vocabulary (number of unique words)\n",
    "# The second argument is the size of the dense vector (embedding dimension)\n",
    "# The third argument is the length of the input sequences (the maximum length of the sentences)\n",
    "model.add(Embedding(vocab_size, dim))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [-0.00630487, -0.0073431 , -0.04714298, -0.01194265,\n",
       "         -0.03146086,  0.0248618 , -0.02722542, -0.03945141,\n",
       "          0.04451882, -0.0379189 ],\n",
       "        [-0.04538488, -0.02755177, -0.01612412,  0.00679729,\n",
       "         -0.04300221,  0.00736079, -0.02943142, -0.04961586,\n",
       "          0.03260313, -0.03274776],\n",
       "        [ 0.03460444, -0.04997913,  0.0356061 ,  0.02159419,\n",
       "          0.04415318,  0.02026075, -0.03621989,  0.00889796,\n",
       "          0.01300943,  0.00113402],\n",
       "        [ 0.01549466, -0.01023472, -0.03196541, -0.04108658,\n",
       "         -0.01947234, -0.0021422 ,  0.0300725 , -0.04186191,\n",
       "          0.04705994, -0.00897392]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [-0.00630487, -0.0073431 , -0.04714298, -0.01194265,\n",
       "         -0.03146086,  0.0248618 , -0.02722542, -0.03945141,\n",
       "          0.04451882, -0.0379189 ],\n",
       "        [-0.04538488, -0.02755177, -0.01612412,  0.00679729,\n",
       "         -0.04300221,  0.00736079, -0.02943142, -0.04961586,\n",
       "          0.03260313, -0.03274776],\n",
       "        [ 0.03460444, -0.04997913,  0.0356061 ,  0.02159419,\n",
       "          0.04415318,  0.02026075, -0.03621989,  0.00889796,\n",
       "          0.01300943,  0.00113402],\n",
       "        [-0.04340411, -0.04093631, -0.02783003, -0.04572313,\n",
       "          0.03568423, -0.04317814, -0.01469032,  0.02273606,\n",
       "          0.01378038,  0.017338  ]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [-0.00630487, -0.0073431 , -0.04714298, -0.01194265,\n",
       "         -0.03146086,  0.0248618 , -0.02722542, -0.03945141,\n",
       "          0.04451882, -0.0379189 ],\n",
       "        [ 0.0492732 , -0.01430202,  0.00710431, -0.03977086,\n",
       "         -0.00424455,  0.01653415, -0.04224806,  0.01223196,\n",
       "          0.04010409, -0.01916954],\n",
       "        [ 0.03460444, -0.04997913,  0.0356061 ,  0.02159419,\n",
       "          0.04415318,  0.02026075, -0.03621989,  0.00889796,\n",
       "          0.01300943,  0.00113402],\n",
       "        [ 0.03548701,  0.03673539,  0.02463939,  0.04600545,\n",
       "          0.00357039, -0.02764603,  0.04642787,  0.01320029,\n",
       "          0.01548349,  0.02597589]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.03497446,  0.01569729,  0.00025153,  0.03277947,\n",
       "         -0.0089236 ,  0.00280025,  0.01819703, -0.02521677,\n",
       "         -0.04900068,  0.02445849],\n",
       "        [ 0.02240975, -0.01825   , -0.02877251,  0.03153774,\n",
       "         -0.02382087, -0.0100014 ,  0.02342692,  0.00590738,\n",
       "          0.04428352, -0.02275659],\n",
       "        [ 0.02214099, -0.02509362,  0.02750936,  0.02916374,\n",
       "          0.00217853,  0.00531311,  0.03136215, -0.00568522,\n",
       "         -0.03153405,  0.02654531],\n",
       "        [ 0.04383415, -0.02076383, -0.04616916,  0.0118974 ,\n",
       "         -0.01162964, -0.03840471,  0.03657514,  0.00608375,\n",
       "         -0.03808872,  0.04538182],\n",
       "        [ 0.04403709,  0.04927236, -0.01967537, -0.00326406,\n",
       "         -0.02331492,  0.01702918,  0.02792276, -0.00602609,\n",
       "         -0.02453208, -0.01919171]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.03497446,  0.01569729,  0.00025153,  0.03277947,\n",
       "         -0.0089236 ,  0.00280025,  0.01819703, -0.02521677,\n",
       "         -0.04900068,  0.02445849],\n",
       "        [ 0.02240975, -0.01825   , -0.02877251,  0.03153774,\n",
       "         -0.02382087, -0.0100014 ,  0.02342692,  0.00590738,\n",
       "          0.04428352, -0.02275659],\n",
       "        [ 0.02214099, -0.02509362,  0.02750936,  0.02916374,\n",
       "          0.00217853,  0.00531311,  0.03136215, -0.00568522,\n",
       "         -0.03153405,  0.02654531],\n",
       "        [ 0.04383415, -0.02076383, -0.04616916,  0.0118974 ,\n",
       "         -0.01162964, -0.03840471,  0.03657514,  0.00608375,\n",
       "         -0.03808872,  0.04538182],\n",
       "        [ 0.03146917, -0.0063541 ,  0.01839006,  0.04106847,\n",
       "         -0.01728851,  0.01363111,  0.02645328,  0.02023312,\n",
       "          0.00203969, -0.01960944]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.03160359,  0.013791  , -0.04735067,  0.01446206,\n",
       "         -0.03133311,  0.02211473,  0.02650762,  0.03038228,\n",
       "         -0.04296431,  0.00226027],\n",
       "        [-0.00630487, -0.0073431 , -0.04714298, -0.01194265,\n",
       "         -0.03146086,  0.0248618 , -0.02722542, -0.03945141,\n",
       "          0.04451882, -0.0379189 ],\n",
       "        [-0.04374783, -0.00559522,  0.0416363 ,  0.00463945,\n",
       "          0.02184807,  0.02688731,  0.00088111,  0.03687674,\n",
       "         -0.02091911,  0.03791595],\n",
       "        [ 0.03460444, -0.04997913,  0.0356061 ,  0.02159419,\n",
       "          0.04415318,  0.02026075, -0.03621989,  0.00889796,\n",
       "          0.01300943,  0.00113402],\n",
       "        [ 0.0076614 , -0.00161303, -0.03704552,  0.0184927 ,\n",
       "          0.03758885, -0.02603813, -0.00461328,  0.03902649,\n",
       "         -0.00021375, -0.03067061]],\n",
       "\n",
       "       [[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [-0.00210899, -0.017302  ,  0.00977749, -0.01777337,\n",
       "          0.01331529,  0.02853341,  0.04526409, -0.03571521,\n",
       "         -0.03052781,  0.02978107],\n",
       "        [-0.04900674, -0.01595354, -0.03326942,  0.0149908 ,\n",
       "         -0.04045798, -0.01741016, -0.04273825,  0.01661471,\n",
       "          0.02796448, -0.03414289],\n",
       "        [ 0.0152528 ,  0.00224277, -0.03731028,  0.02870896,\n",
       "         -0.0451421 ,  0.0245034 , -0.04515746,  0.0288373 ,\n",
       "         -0.02311058, -0.04166694],\n",
       "        [ 0.04383415, -0.02076383, -0.04616916,  0.0118974 ,\n",
       "         -0.01162964, -0.03840471,  0.03657514,  0.00608375,\n",
       "         -0.03808872,  0.04538182]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(embedded_docs)\n",
    "# The output is a 3D array with shape (number of samples, sequence length, embedding dimension)\n",
    "# each list has 10 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the Embedding layer converts all integer tokens in the input sequence, including the padding token (0), into dense vectors. This is because the Embedding layer maps every integer index (from 0 to vocab_size - 1) to a corresponding dense vector in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0 6910 6898 6426 6741]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [ 0.04989907, -0.03139565,  0.04958474, -0.00162212,\n",
       "          0.0269662 , -0.02546694, -0.02383177,  0.00211507,\n",
       "          0.00839097, -0.01155689],\n",
       "        [-0.00630487, -0.0073431 , -0.04714298, -0.01194265,\n",
       "         -0.03146086,  0.0248618 , -0.02722542, -0.03945141,\n",
       "          0.04451882, -0.0379189 ],\n",
       "        [-0.04538488, -0.02755177, -0.01612412,  0.00679729,\n",
       "         -0.04300221,  0.00736079, -0.02943142, -0.04961586,\n",
       "          0.03260313, -0.03274776],\n",
       "        [ 0.03460444, -0.04997913,  0.0356061 ,  0.02159419,\n",
       "          0.04415318,  0.02026075, -0.03621989,  0.00889796,\n",
       "          0.01300943,  0.00113402],\n",
       "        [ 0.01549466, -0.01023472, -0.03196541, -0.04108658,\n",
       "         -0.01947234, -0.0021422 ,  0.0300725 , -0.04186191,\n",
       "          0.04705994, -0.00897392]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first padded sequence (integer-encoded representation of the first sentence)\n",
    "print(embedded_docs[0])\n",
    "\n",
    "# Predict the dense vector representation of the first padded sequence using the Embedding layer\n",
    "# The output is a 3D array where:\n",
    "# - The first dimension is the batch size (1 in this case)\n",
    "# - The second dimension is the sequence length (sent_length)\n",
    "# - The third dimension is the embedding size (dim)\n",
    "model.predict(embedded_docs[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
